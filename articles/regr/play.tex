\documentclass[11pt]{article}
\usepackage{enumitem}
%\usepackage[T1]{fontenc}
\usepackage[auth-sc,affil-sl]{authblk}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{color}
%\usepackage{enumerate}
\usepackage[round]{natbib}
%\usepackage{url} % not crucial - just used below for the URL 
%\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{alltt}
\usepackage{listings}
\usepackage{array}
\usepackage[noline, boxed, linesnumbered, procnumbered, titlenumbered]{algorithm2e}
%\usepackage[firstpage]{draftwatermark}
\usepackage[margin=1in]{geometry}  %%jcgs has own margins
\usepackage{lmodern}
\usepackage{caption}
\usepackage{subcaption}
\newcommand{\spd}{\fontfamily{cmr}\textsc{\small StratPD}}
\newcommand{\cspd}{\fontfamily{cmr}\textsc{\small CatStratPD}}
\newcommand{\xnc}{$x_{\overline{c}}$}
\newcommand{\xnC}{$x_{\overline{C}}$}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\textwidth = 6.5 in
\textheight = 9 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\topmargin = 0.0 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}

\title{Playground}
\author{parrt}
\begin{document}
\maketitle

\begin{abstract}
Model interpretability is important to machine learning practitioners and a key component of  interpretation is the characterization of partial dependence of the response on any subset of features used in the model. The two most common strategies suffer from a number of critical weaknesses. First,  linear regression model coefficients describe how a unit change in an explanatory variable changes the response, while holding other variables constant. But, linear regression is inapplicable for high dimensional, $p>n$, data sets or when a linear model is insufficient to capture the relationship between explanatory variables and the response. Second, Partial Dependence (PD) plots and Individual Conditional Expectation (ICE) plots give biased results for the common situation of codependent variables and they rely on fitted models provided by the user. When the supplied model is a poor choice due to systematic bias or overfitting, PD/ICE plots provide little (if any) useful information.  

To address these issues, we introduce a new strategy, called \spd{}, that does not depend on a user's fitted model, provides accurate results in the presence codependent variables, and is applicable to high dimensional settings. The strategy works by stratifying a data set into groups of observations that are similar, except in the variable of interest, through the use of a decision tree. Any fluctuations of the response variable within a group is likely due to the variable of interest. We apply \spd{} to a collection of simulations and case studies to show that \spd{} is a fast, reliable, and robust method for assessing partial dependence with clear advantages over state-of-the-art methods. 
\end{abstract}

\pagebreak	
Examine CatStrat algorithm.

\setlength{\algomargin}{5pt}
\begin{algorithm}[]
\DontPrintSemicolon
\LinesNumbered
\SetAlgorithmName{Algorithm}{List of Algorithms}
\SetAlgoSkip{}
\SetInd{.5em}{.5em}
\TitleOfAlgo{{\em StratPD}}
\KwIn{$\begin{array}[t]{l}
{\bf X}, {\bf y}, c,\\
{\it ntrees=1}, {\it bootstrap=false}, {\it max\_split\_features=all},\\
{\it min\_samples\_leaf}, {\it nbins}\\
\end{array}$
}
\KwOut{collection of $\beta_R$ coefficients across $x_c$, partial dependence curve}
Train random forest regressor $\it rf$ on (\xnc{}, $\bf y$) with hyper-parameters:\\
~~~~~$\it ntrees$, $\it bootstrap$, $\it max\_split\_features$, ${\it min\_samples\_leaf}$ \\
\ForEach{tree $T \in \it rf$}{
    \ForEach{leaf $L \in T$}{
    	$(x^{(L)}, y^{(L)})$ = $\{(x_{ic},  y_i)\}_{i \in L}$\\
%	$R^{(L)} = [min(x^{(L)}),max(x^{(L)})]$\\
	$bins$ = split range $[min(x^{(L)}),max(x^{(L)})]$ into $nbins$ bins\\
    	\ForEach{bin $B \in bins$}{
    		$(x^{(B)}, y^{(B)})$ = $\{(x_{ic},  y_i)\}_{i \in B}$\\
    		$R^{(B)}$ = $[min(x^{(B)}), max(x^{(B)})]$\\
                 $n^{(B)} = \begin{cases} |B| &\mbox{if } width(R^{(B)}) > 0.0 \\ 
                   0 & otherwise\end{cases}$~~~~~~~({\it Assume $L$ is unique id across trees})\\
%    		\lIf{left$(R_{B})$ = right$(R_{B})$}{{\bf continue}}~~~~~({\it Ignore leaves w/o change in $x_c$})\\
    		Fit linear model to $(B_x, B_y)$ giving $\beta_{B}$\\
	}
    }
}
$n = \Sigma_{T \in {\it rf}} \Sigma_{L \in T} \Sigma_{B \in L} n^{(B)}$~~~~~~~~~~~~~~~~~~~~~~~~~~~~({\it Num observations supporting $\beta_B$ computations})\\
$uniqx$ = sorted(unique($x_c$))\\
\For{$i=1$ {\bf to} $|uniqx|-1$}{
	$R$ = $(uniqx_i, uniqx_{i+1})$\\
	\ForEach{bin $B$ created above}{
		$\beta_R = \frac{1}{n}\Sigma_{B \in R}|B|\beta_{B}$
	}
}
$pd$ = numerically integrate $\beta_R$'s across $uniqx$\\
\Return{collection of all $\beta_R$, $pd$}
\label{alg:StratPD}
\end{algorithm}


\setlength{\algomargin}{5pt}
\begin{algorithm}[]
\DontPrintSemicolon
\LinesNumbered
\SetAlgorithmName{Algorithm}{List of Algorithms}
\SetAlgoSkip{}
\SetInd{.5em}{.5em}
\TitleOfAlgo{{\em CatStratPD}}
\KwIn{$\begin{array}[t]{l}
{\bf X}, {\bf y}, c,\\
{\it ntrees=1}, {\it bootstrap=false}, {\it split\_features=all}, {\it min\_samples\_leaf}\\
\end{array}$
}
\KwOut{$\begin{array}[t]{l}
\Delta^{(k)} = \text{category } k \text{'s effect on } y \text{ where } mean(\Delta^{(k)})=0\\
n = \text{number of supported observations}\\
\end{array}$
}
Train random forest regressor $\it rf$ on (\xnc{}, $\bf y$) with hyper-parameters:\\
~~~~~$\it ntrees$, $\it bootstrap$, $\it split\_features$, ${\it min\_samples\_leaf}$ \\
\ForEach{tree $T \in \it rf$}{
    \ForEach{leaf $L \in T$}{
        Let $(x^{(L)}, y^{(L)})$ = $\{(x_{ic},  y_i)\}_{i \in L}$\\
        Let $n_x^{(L)}=|unique(x^{(L)})|$\\
%        Let $K=\{k_1, k_2, \,\dots, k_{n_x}\}$ where \\
        ${\bf y}^{(L,k)} = y^{(L)}[x^{(L)}=k]$~~~~~~~~~~~~~({\it Group leaf $x_c$ by category $k$})\\
        $n^{(L,k)} = \begin{cases} |{\bf y}^{(L,k)}| &\mbox{if } n_x > 1 \\ 
        0 & otherwise\end{cases}$\\
        $\overline{y}^{(L,k)} = \frac{1}{|{\bf y}^{(L,k)}|} \Sigma_{i=1}^{|{\bf y}^{(L,k)}|} y_i^{(L,k)}$ ~~~~\,~({\it Mean of leaf $y^{(L)}$ for category $k$})\\
        $\Delta^{(L,k)} = \overline{y}^{(L,k)} - \overline{y}^{(L)}$ ~~~~~~~~~~~~~({\it Remove contribution of $x_{\overline{c}}$ to $y^{(L)}$})\\
    }
}
$n^{(k)} = \Sigma_{T \in {\it rf}} \Sigma_{L \in T} ~n^{(L,k)}$~~~~~~~~~~~~~~~~~~~~~~({\it Num supporting observations for $k$})\\
%$\Delta^{(k)} = \frac{1}{|n^{(k)}|} \Sigma_{T \in {\it rf}} \Sigma_{L \in T} \Sigma_{i=1}^{|{\bf y}^{(L,k)}|} ~ y_i^{(L,k)} \mathbf{1}_{n_x^{(L)}>1}$~~~~~({\it Average change in $y$ for $k$ across leaves})\\
$\Delta^{(k)} = \frac{1}{n^{(k)}} \Sigma_{T \in {\it rf}} \Sigma_{L} \,n^{(L,k)}\Delta^{(L,k)}$~~~~~~~~~~({\it Change for $k$ is weighted, averaged across leaves})\\
\Return{$\{{\Delta}^{(1)}, {\Delta}^{(2)}, \,\ldots, {\Delta}^{(k)}\}$}, $n$
\label{alg:CatStratPD}
\end{algorithm}

 \end{document}