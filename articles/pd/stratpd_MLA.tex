\documentclass[3p,times]{elsarticle}
%\documentclass[preprint,3p,12pt,authoryear]{elsarticle}
%\documentclass[final,3p,times,onecolumn,times]{elsarticle}

%% The `ecrc' package must be called to make the CRC functionality available
\usepackage{ecrc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\setcitestyle{authoryear,round}
\usepackage{natbib}
\renewcommand{\bibnumfmt}[1]{}
\usepackage{amsmath}
\usepackage{array}
\usepackage{url}
\usepackage{graphicx,psfrag,epsf, amsmath}
\usepackage{enumerate}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage[noline, titlenumbered, vlined]{algorithm2e}
%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}
%\setcitestyle{numbers,open={[},close={]}}
\usepackage{epstopdf,gensymb}
\usepackage{graphics}
\usepackage{graphicx,color}
\newcommand{\erdos}{Erd\H{o}s-R\'enyi }
%\usepackage{endfloat}
\usepackage{amsfonts, animate,subfigure}
\usepackage{geometry}


\usepackage{indentfirst}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{bm,pbox}
\usepackage{multirow}
\usepackage{color}
\usepackage{framed}
\usepackage{amssymb,amsthm,amsfonts,amsbsy,latexsym,dsfont}
\usepackage{enumerate}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %% Local environments
 \newtheorem{theorem}{Theorem}
 \newtheorem{lemma}[theorem]{Lemma}
 \newtheorem{proposition}[theorem]{Proposition}
 \newtheorem{claim}[theorem]{Claim}
 \newtheorem{fact}[theorem]{Fact}
 \newtheorem{corollary}[theorem]{Corollary}
 \newtheorem{definition}[theorem]{Definition}
 \newtheorem{observation}[theorem]{Observation}
 \newtheorem{remark}[theorem]{Remark}
 \newtheorem*{problem}{Problem}
 \newtheorem{property}[theorem]{Property}

\newcommand{\secref}[1]{Section~\ref{#1}}
\renewcommand{\eqref}[1]{Equation~\ref{#1}}
\newcommand{\tblref}[1]{Table~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\algref}[1]{Algorithm~\ref{#1}}
\newcommand{\funref}[1]{Function~\ref{#1}}
\newcommand{\listingref}[1]{Listing~\ref{#1}}
\usepackage{setspace}

\newcommand{\eg}{{\em e.g.}}
\newcommand{\ith}{$i^{th}$}
\newcommand{\cut}[1]{}
\newcommand{\todo}[1]{{{\color{red}{#1}}}}

\newcommand{\spd}{\fontfamily{cmr}\textsc{\small StratPD}}
\newcommand{\cspd}{\fontfamily{cmr}\textsc{\small CatStratPD}}
\newcommand{\xnc}{${\bf X}_{\overline{c}}$}
%\newcommand{\xnj}{${\bf X}_{\overline{j}}$}
\newcommand{\Xnj}{${\bf X}_{\texttt{\char`\\} j}$}
\newcommand{\xnj}{$x_{\texttt{\char`\\} j}$}
\newcommand{\xnC}{$x_{\overline{C}}$}
\renewcommand{\xi}{x^{(i)}}
\newcommand{\yi}{y^{(i)}}
\renewcommand{\slash}{\texttt{\char`\\}}

\DeclareMathOperator{\Ex}{\mathbb{E}}

%\makeatletter
%\providecommand{\doi}[1]{%
%  \begingroup
%    \let\bibinfo\@secondoftwo
 %   \urlstyle{rm}%
%    \href{http://dx.doi.org/#1}{%
%      doi:\discretionary{}{}{}%
%      \nolinkurl{#1}%
%    }%
%  \endgroup
%}
%\makeatother


 %%%%%%%%%%%%%%%%%%%%%%%%%%%%
 %%Trying to fix the moving table problem
 \usepackage{etoolbox}
 
%% The ecrc package defines commands needed for running heads and logos.
%% For running heads, you can set the journal name, the volume, the starting page and the authors

%% set the volume if you know. Otherwise `00'
\volume{00}

%% set the starting page if not 1
\firstpage{1}

%% Give the name of the journal
\journalname{Machine Learning with Applications}

%% Give the author list to appear in the running head
%% Example \runauth{C.V. Radhakrishnan et al.}
\runauth{Terence Parr and James D. Wilson}

%% The choice of journal logo is determined by the \jid and \jnltitlelogo commands.
%% A user-supplied logo with the name <\jid>logo.pdf will be inserted if present.
%% e.g. if \jid{yspmi} the system will look for a file yspmilogo.pdf
%% Otherwise the content of \jnltitlelogo will be set between horizontal lines as a default logo

%% Give the abbreviation of the Journal.
\jid{MLA}

%% Give a short journal name for the dummy logo (if needed)
\jnltitlelogo{\small Machine Learning with Applications}

%% Hereafter the template follows `elsarticle'.
%% For more details see the existing template files elsarticle-template-harv.tex and elsarticle-template-num.tex.

%% Elsevier CRC generally uses a numbered reference style
%% For this, the conventions of elsarticle-template-num.tex should be followed (included below)
%% If using BibTeX, use the style file elsarticle-num.bst

%% End of ecrc-specific commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

% if you have landscape tables
\usepackage[figuresright]{rotating}

% put your own definitions here:
%   \newcommand{\cZ}{\cal{Z}}
%   \newtheorem{def}{Definition}[section]
%   ...

% add words to TeX's hyphenation exception list
%\hyphenation{author another created financial paper re-commend-ed Post-Script}

% declarations for front matter

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

%\dochead{}
%% Use \dochead if there is an article header, e.g. \dochead{Short communication}

\title{Partial Dependence through Stratification}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

%\author{Jihui Lee $^a$\corref{cor1}, Gen Li $^a$, and James D. Wilson $^b$}
%\address{$^a$ Department of Biostatistics, Columbia University \\ 
%$^b$ Department of Mathematics and Statistics, University of San Francisco}

\author[col]{Terence Parr}
\author[wcm]{James D. Wilson\corref{cor1}}

\address[col]{Master of Data Science Program, University of San Francisco. San Francisco, CA 94117, USA. Email: parrt@cs.usfca.edu}

\cortext[cor1]{Corresponding author}
\address[wcm]{Department of Mathematics and Statistics, University of San Francisco. San Francisco, CA 94117, USA. Email: jdwilson4@usfca.edu}

\begin{abstract}
Partial dependence curves (FPD) are commonly used to explain feature importance once a supervised learning model has been fitted to data. However, it is common for the same partial dependence algorithm to give meaningfully different curves for different supervised models, even when the algorithm is applied to the same data. As a result, it is difficult to distinguish between model artifacts and true relationships in the data. In this paper, we contribute methods for computing partial dependence curves, for both numerical (\spd) and categorical explanatory variables (\cspd), that work directly from training data rather than the predictions of a fitted model. Our methods provide a direct estimate of partial dependence, and rely on approximating the partial derivative of an unknown regression function. We investigate settings where contemporary partial dependence methods --- including FPD,  Accumulated Local Effects (ALE), and SHapley Additive exPlanations (SHAP) methods --- give biased results. We demonstrate that our approach works correctly on synthetic data and plausibly on real data sets. This work motivates a new line of inquiry into nonparametric partial dependence that provides robust information about the variables considered in a supervised learning task.
\\ \\
\end{abstract}

\begin{keyword}
decision trees; feature importance; random forests; variable importance
\end{keyword}

\end{frontmatter}

\section{Introduction}
\onehalfspacing

When choosing a supervised model that relates feature and response pairs, model interpretability is often at odds with predictive power. Indeed, these two objectives have traditionally led to the choice of either an interpretable \emph{or} a predictive model (see for example \cite{shmueli2010explain}). This choice has largely been dictated by machine learning and statistics cultures \citep{breiman2001statistical, donoho201750}, where machine learning practitioners focus on predictive ability and statistical practitioners focus on interpretability and inference. Recently, however, there has been a shift in the division of these two objectives as the machine learning community has begun to build what are being called  ``interpretable machine learning'' models \citep{doshi2017towards, vellido2012making}. Interpretable machine learning models aim to get the best of both worlds by achieving high predictive power and ensuring that the predictions of the model can be easily interpreted. 

In practice machine learning model interpretation is just as important as, and in many cases more important than, obtaining a highly predictive model. Interpretable models play an essential role in artificial intelligence \citep{adadi2018peeking}, where the interpretation of models with high predictive power like neural networks and deep learning techniques are essential to ensure robust, replicable outcomes. Interpretable models are also at the heart of applications in medicine such as precision medicine as well as psychology and psychiatry, where models are used to describe the relationship between an individual's demographic and clinical features and their susceptibility to illness and disease, among other measureable outcomes \citep{dwyer2018machine, katuwal2016machine}.

A key component of model interpretation involves the characterization of the partial dependence of the response on any subset of the features used in the model. For example, consider the aforementioned example of precision medicine. The success of a prevention plan for a ``high-risk" individual depends on their disease susceptibility as well as their clinical characteristics (e.g., age and co-morbidities). That is, the effect that these characteristics have on the person's susceptibility guides the development of a prevention plan and often times is more important than the accuracy of predicting one's chances of the disease itself."  Business analysts at a car manufacturer might need to know how changes in their supply chain affect defect rates. Climate scientists are interested in how different atmospheric carbon levels affect temperature.  {\em Partial dependence}, the isolated effect of a specific variable or variables on the response variable, $y$, is important to researchers and practitioners in many disparate fields.

In the simplest setting when there is only one explanatory variable, $x_1$, a plot of response $y$ against $x_1$ visualizes the marginal effect of feature $x_1$ on $y$ exactly. Given two or more features, one can similarly plot the marginal effects of each feature separately, however, the analysis is complicated by the interactions of the variables \citep{cox2014multivariate}. Variable interactions and codependencies between features result in marginal plots that do not isolate the specific contribution of a feature of interest to the response. For example, a marginal plot of sex (male/female) against body weight would likely show that, on average, men are heavier than women. While true, men are also taller than women on average, which likely accounts for most of the difference in average weight. It is unlikely that two ``identical'' people, differing only in sex, would be appreciably different in weight.  

In general, strategies for partial dependence seek to characterize the individual and joint effects of the variables in the columns of an explanatory matrix $\mathbf{X} \in \mathcal{R}^{n \times p}$ on a response vector $\mathbf{y} \in \mathcal{R}^n$. Contemporary partial dependence techniques such as Friedman's original partial dependence \citep{PDP} (which we will denote FPD), functional ANOVA \citep{fanova}, Individual Conditional Expectations (ICE) \citep{ICE}, Accumulated Local Effects (ALE) \citep{ALE}, and most recently SHAP \citep{shap} interrogate variable importance through fitted models. Model-based techniques like these assess partial dependence by (1) first estimating an unknown model $f: \mathbf{X} \rightarrow \mathbf{y}$ with $\hat{f}$ that characterizes the relationship between $\mathbf{X}$ and $\mathbf{y}$ and (2) subsequently quantifying the effect of each feature $x_j$ by marginalizing $\hat{f}$ (specific details of these methods are provided in \secref{sec:related}). Model-based techniques dominate the partial dependence research literature because interpreting the output of a fitted model  has several advantages. Perhaps the most important consideration is that models generally smooth over noise. Models thereby act like analysis preprocessing steps, potentially reducing the computational burden of the technique; e.g., ALE is $O(n)$ for the $n$ records of $\bf X$. Model-based techniques are typically model-agnostic, though for efficiency, some provide model-specific optimizations, as SHAP does. Partial dependence techniques that interrogate models also provide insight into the models themselves; i.e., how variables affect model behavior.  It is also true that, in some cases, a predictive model is the primary goal so creating a suitable model is not an extra burden.

Despite their wide use, model-based techniques have two significant disadvantages. 
The first relates to their ability to tease apart the effect of codependent features. It is often the case that models are sometimes required to extrapolate into regions of nonexistent support or even into nonsensical observations (see discussions in \citet{ALE} and \citet{fanova}).  As we demonstrate in our numerical studies in \secref{sec:experiments}, model-based techniques can vary in their ability to isolate variable effects in practice. Second, hazards exist in the estimation of the true function that relates $\mathbf{X}$ to $\mathbf{y}$. If a fitted model is unable to accurately capture the relationship between features and $y$, for whatever reason, then partial dependence does not provide any useful information to the user.  To make interpretation more challenging, there is no definition of ``accurate enough.'' A user's understanding of partial dependence comes down to the model that they fit. As there is no ``ground-truth'' model or algorithm, and partial dependence analyses are often not robust across models, analyses depend almost entirely on the model that a practitioner utilizes. Also, given an accurate fitted model, business analysts and scientists peer at the data through the lens of the model, which can distort partial dependence curves. Separating visual artifacts of the model from real effects present in the data requires expertise in model behavior (and optimally in the implementation of model fitting algorithms). 

Consider the combined FPD/ICE plots shown in \figref{fig:baths_price} derived from several models (random forest, gradient boosting, linear regression, deep learning) fitted to the same New York City rent data set from \citet{rent}.  (The rent data set includes 14 features about an apartment with a response variable indicating the interest level in the associated apartment webpage.) The subplots in \figref{fig:baths_price}(b)-(e)  present starkly different partial dependence relationships and it is unclear which, if any, is correct.  The marginal plot, (a), drawn directly from the data shows a roughly linear growth in price for a rise in the number of bathrooms, but this relationship is biased because of the dependence of bathrooms on other variables, such as the number of bedrooms. (e.g., five bathroom, one bedroom apartments are unlikely.)  For real data sets with codependent features, the true relationship is unknown so it is hard to evaluate the correctness of the plots. (Humans are unreliable estimators, which is why we need data analysis algorithms in the first place.) Nonetheless, having the same algorithm, operating on the same data, give meaningfully different partial dependences is undesirable and makes one question their precision.

\begin{figure}[H]
\begin{center}
\includegraphics[width = \textwidth]{images/bathrooms_vs_price.pdf}\vspace{-3mm}
\caption{Meaningfully different results from a single partial dependence technique, FPD/ICE, applied to the same data but different models. Plots of number of bathrooms versus rent price using New York City apartment rent data \citep{rent} with $n=10,000$ of \textasciitilde50k. (a) marginal plot, (b) plot derived from random forest, (c)  plot derived from gradient boosted machine, and (d) plot derived from ordinary least squares regression. Hyper parameters were tuned using 5-fold cross validation grid search over several hyper parameters. (e) A Keras model trained by experimentation: single hidden layer of 100 neurons, 500 epochs, batch size of 1000, batch normalization, and 30\% dropout. (f) \spd{} gives a plausible result that rent goes up linearly with the number of bathrooms. $R^2$ were computed on 20\% validation sets.\vspace{-7mm}}
\label{fig:baths_price}
\end{center}
\end{figure}

Experts are often able to quickly recognize model artifacts, such as the  stairstep phenomenon in \figref{fig:baths_price}(b) and (c) inherent to decision tree-based methods trying unsuccessfully to extrapolate.  In this case, though, the stairstep is more accurate than the linear relationship in (d) and (e) because the number of bathrooms is discrete (except for ``half baths'').  The point is that interpreting model-based partial dependence plots can be misleading, even for experts.

An accurate mechanism to compute partial dependences that did not peer through fitted models would be most welcome. Such partial dependence curves would be accessible to business analysts and scientists who lack the expertise to choose, tune, and assess models (or write software at all). (One can imagine a spreadsheet plug-in that produced partial dependence curves.) A mechanism that did not rely on a user-provided model would also reduce the chance of plot misinterpretation due to model artifacts and could even help machine learning practitioners to choose appropriate models based upon relationships exposed in the data.  Such a mechanism, and the associated open-source Python library, is the key contribution of this paper.

In this paper, we propose a strategy, called {\textsc{strat}ified \textsc{p}artial \textsc{d}ependence} (\spd{}), that computes partial dependences directly from training data $({\bf X}, {\bf y})$, rather than through the predictions of a fitted model. Our technique is based upon the notion of an idealized partial dependence:  integration over the partial derivative of $y$ with respect to the variable of interest for the smooth function that generated $({\bf X}, {\bf y})$. As that function is unknown, we estimate the partial derivatives from the data non-parametrically. Informally, the approach examines changes in $y$ across $x_j$ while holding \xnj{} constant or nearly constant (\xnj{} denotes all variables except $x_j$). To hold \xnj{} constant, we use a single decision tree to partition feature space, a concept used by \citet{rfimp} and \citet{RFunsup} for conditional permutation importance and observation similarity measures, respectively. We furthermore develop \cspd{}, a stratification technique that computes partial dependence curves for categorical variables that, unlike existing techniques, does not assume adjacent category levels are similar. Both \spd{} and \cspd{} are quadratic in $n$, in the worst case (like FPD), though \spd{} behaves linearly on real data sets. Both \spd{} and \cspd{} are constructed specifically for regression where the the response ${\bf y}$ is assumed to be continuous on $\mathbf{R}^n$. Furthermore both strategies effectively handle mutually dependent features. We show the unique result of \spd{} when applied to the same example considered in Figure 1 in plot 1(f). In this example, \spd{} provides a plausible interpretation of the New York City appartment rent data, namely that rent increases linearly with the number of bathrooms.

% isolates only single-variable partial dependence, and cannot identify interaction effects (as ICE can).  



We begin by describing the proposed stratification approach in \secref{sec:stratpd}. We compare \spd{} to related (model-based) work in \secref{sec:related}. In \secref{sec:experiments}, we present partial dependence curves generated by \spd{} and \cspd{} on synthetic and real data sets, contrast these results with existing methods, and use synthetic data to highlight advantages of \spd{} and \cspd{} over contemporary model-based methods. Software and reproducible scripts for the algorithm and all results in this manuscript are available via the Python package \texttt{stratx} with source code at \url{github.com/parrt/stratx}.

% ------------------------------------------------------------------------------------------------------------------------------------
\section{Partial dependence without model predictions}\label{sec:stratpd}

Assume we are given training data ($\bf X, y$) where ${\bf X} = [x^{(1)}, \ldots, x^{(n)}]$ is an $n \times p$ matrix whose $p$ columns represent observed features and ${\bf y}$ is the $n \times 1$ vector of responses. For any smooth function $f:\mathbb{R}^{p} \rightarrow \mathbb{R}$ that precisely maps each $\xi$ row vector to response $y^{(i)}$, ${y^{(i)}} = f(\xi)$, the partial derivative of $y$ with respect to $x_j$ gives the change in $y$ holding all other variables constant.  Integrating the partial derivative then gives the {\em idealized partial dependence}  of $y$ on $x_j$, the isolated contribution of $x_j$ to $y$:

\noindent {\bf Definition 1} The {\em idealized partial dependence} of $y$ on feature $x_j$ for continuous and smooth generator function $f:\mathbb{R}^{p} \rightarrow \mathbb{R}$ evaluated at $x_j = z$ is the cumulative sum up to $z$:\vspace{-1mm}

\begin{equation}\label{eq:pd}
\text{\it PD}_j(z) = \int_{min(x_j)}^z \frac{\partial f}{\partial x_j} dx_j
\end{equation}\vspace{-1mm}

\noindent $\text{\it PD}_j(z)$ is the value contributed to $f$ by $x_j$ at $x_j = z$ and $\text{\it PD}_j(min(x_j))=0$. The underlying generator function is unknown, and so previous approaches begin by estimating $f$ with a fitted model, $\hat{f}$. Instead, we estimate the partial derivatives of the true function, ${\partial f}/{\partial x_j}$, from the raw training data then integrate to obtain $PD_j$. The advantages of this $PD_j$ definition are that it ({\em i}) does not require a fitted model or its predictions and ({\em ii}) is insensitive to codependent features.  

In recent work, ALE also derived partial dependence by estimating and integrating across partial derivatives (see Equation 7 in \citealt{ALE}) but did so by first estimating $\hat{f}$ and then calculating $PD_j$. The notable difference is that our approach directly estimates ${\partial f}/{\partial x_j}$ without peering through the intermediate $\hat{f}$ approximation. We show in Section 4 and 5 where our algorithm can have significant advantages in real and simulated data.

\subsection{A stratification approach to partial dependence}
The aim of \spd{} is to estimate $PD_j$ for each feature $x_j$ across its domain. There are two primary steps in the algorithm, described below. Psuedo-code for both \spd{} and \cspd{} are provided in the Appendix in Algorithm 1 and Algorithm 2, respectively.\\

\noindent \underline{\bf Step 1}: First, the \xnj{} feature space is partitioned into disjoint regions of observations where all \xnj{} variables are approximately matched across the observations in that region. Stratification occurs through the use of a decision tree fit to (\Xnj, $\bf y$). The leaves of the decision tree aggregate observations with equal or similar \xnj{} features. The \xnj{} features can be numerical variables or label-encoded categorical variables (assigned a unique integer). \spd{} only uses the tree for the purpose of partitioning feature space and never uses predictions from any model. This use of decision trees is closely related to the work from \citet{rfimp} and \citet{RFunsup} for conditional permutation importance and observation similarity measures, respectively.\\

\noindent \underline{\bf Step 2}: Once each variable has been partitioned via stratification, \spd{} next estimates ${\partial f}/{\partial x_j}$ and $PD_j(z)$ through averaging. Within each \xnj{} region, any fluctuations in the response variable are likely due to the variable of interest, $x_j$.  In the ideal case, the values for each \xnj{} variable within a region are identical and, because only $x_j$ is changing, $y$ changes should be attributed to $x_j$, noise, or irreducible error. Estimates of the partial derivative within a region are computed discretely as the changes in $y$ values between unique and ordered $x_j$ positions:  $(\bar{y}^{(i+1)} - \bar{y}^{(i)})/(x_j^{(i+1)} - x_j^{(i)})$ for all $i$ in a region such that $x_j^{(i)}$ is unique and $\bar{y}^{(i)}$ is the average $y$ at unique $x_j^{(i)}$.  This amounts to performing piecewise linear regression through the region observations, one model per unique pair of $x_j$ values, and collecting the model $\beta_1$ coefficients to estimate partial derivatives. The overall partial derivative at $x_j=z$ is the average of all slopes, found in any region, whose range $[x_j^{(i)},x_j^{(i+1)})$ spans $z$.  One could apply a nonparametric method to smooth through the discontinuities at $x_j$ points within a leaf, but this has not proven necessary in practice.  The partial dependence curve points are often the result of two averaging operations, one within and one across regions, which tends to smooth the curve.  As an illustration of stratification leading to estimates of partial derivatives using finite differences, consider \figref{fig:stratification} (also see \figref{fig:interactions}(d) below).

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width = 280pt]{images/sample-stratification.pdf}\vspace{-3mm}
\caption{An illustration of stratification of $x_1,x_2$ into three groups of equal values. The three groups are represented by three connected line segment groups in the $x_3$ versus $y$ plot. Each adjacent pair of $x_3$ points within a group yields a finite difference for that $x_3$ range. Each orange bar indicates a region with a slope estimate. The overall slope estimate at $x_3$ point is the average slope from all overlapping regions. For example, between 0 and 1, the slope estimate is just 2, but between 1.1 and 2.4 the slope estimate is the average of 2 and -3.85.}
\label{fig:stratification}
\end{center}
\end{figure}

For the stratification approach to work, decision tree leaves must satisfactorily stratify \xnj{}. If the \xnj{} observations in each region are not similar enough, the relationship between $x_j$ and $y$   is less accurate.  Regions can also become so small that even the $x_j$ values become equal, leaving a single unique $x_j$ observation in a leaf. Without a change in $x_j$, no partial derivative estimate is possible and these nonsupporting observations must be  ignored (e.g., the two leftmost points in \figref{fig:partitioning}(a)). A degenerate case occurs when identical or nearly identical $x_j$ and $x_j'$ variables exist. Stratifying $x_j$ as part of \xnj{} would also match up $x_j'$ values, leading to both exhibiting flat curves, as if the decision tree were trained on $(\bf X, y)$ not (\Xnj, $\bf y$). Our experience is that using the collection of leaves from a random forest, which restricts the number of variables available during node splitting, prevents partitioning from relying too heavily on either $x_j$ or $x_j'$. Some leaves have observations that vary in $x_j$ or $x_j'$ and partial derivatives can still be estimated. It is worth noting that only a decision tree based technique is applicable to our method because no other model stratifies feature space into sets of roughly similar features (each leaf contains records with similar \xnj{} features).

\spd{} uses a hyper parameter, {\tt\small min\_samples\_leaf}, to control the minimum number of observations in each decision tree leaf.  Generally speaking, smaller values lead to more confidence that fluctuations in $y$ are due solely to $x_j$, but more observations per leaf 
prevent \spd{} from missing nonlinearities and make it less susceptible to noise.  As the leaf size grows, however, one risks introducing contributions from \xnj{} into the relationship between $x_j$ and $y$. At the extreme, the decision tree would consist of a single leaf node containing all observations, leading to a marginal not partial dependence curve.

\spd{} uses another hyper parameter called {\tt\small min\_slopes\_per\_x} to ignore any partial derivatives estimated with too few observations.  Dropping uncertain partial derivatives greatly improves accuracy and stability. Partial dependences computed by integrating over local partial derivatives are highly sensitive to partial derivatives computed at the left edge of any $x_j$'s range because imprecision at the left edge affects the entire curve.  This presents a problem when there are few samples with $x_j$ values at the extreme left (see, for example, the $x_j$ histogram of \figref{fig:yearmade}(d)).  Fortunately, sensible defaults for \spd{} (10 observations and 5 slopes) work well in most cases and  were used to generate all plots in this paper.

For categorical explanatory variables, \cspd{} uses the same stratification approach, but cannot apply regression of $y$ to non-ordinal, categorical $x_j$. Instead, \cspd{} groups leaf observations by category and computes the average response per category in each leaf. Consider a single leaf and its  $p$-dimensional average response vector $\bar{\bf y}$. Then choose a random reference category, {\it refcat}, and subtract that category's  average value from $\bar{\bf y}$ to get a vector of relative deltas between categories: $\Delta {\bf y}$ = $\bar{\bf y} - \bar{\bf y}_{\it refcat}$. The $\Delta  {\bf y}$ vectors from all leaves are then merged via averaging, weighted by the number of observations per category, to get the overall effect of each category on the response.  The delta vectors for two leaves, $\Delta {\bf y}$ and $\Delta {\bf y}'$, can only be merged if there is at least one category in common.  \cspd{} initializes a running average vector to an arbitrary starting leaf's $\Delta  {\bf y}$ and then makes multiple passes over the remaining vectors, merging any vectors with a category in common with the running average vector.  Observations associated with any remaining, unmerged leaves must be ignored. \cspd{} uses a single hyper parameter {\tt\small min\_samples\_leaf} to control stratification. Both \spd{} and \cspd{} have an optional hyper parameter called {\tt\small ntrials} (default is 1) that averages the results from multiple bootstrapped samples, which can reduce variance.

Stratification of high-cardinality categorical variables tends to create small groups of category subsets, which complicates the averaging process across groups. (Such $\Delta {\bf y}$ vectors are sparse and we use {\it NaN}s to represent missing values.) If both groups have the same reference category, merging is a simple matter of averaging the two delta vectors, where {\it mean}({\it z,NaN}) = {\it z}.  For delta vectors with different reference categories and at least one category in common, one vector is adjusted to use a randomly-selected reference category, $c$, in common: $\Delta {\bf y}' = \Delta {\bf y}' - \Delta {\bf y}_c' + \Delta {\bf y}_c$. That equation adjusts vector $\Delta {\bf y}'$ so $\Delta {\bf y}_c'=0$ then adds the corresponding value from $\Delta {\bf y}$ so $\Delta {\bf y}_c' = \Delta {\bf y}_c$, which renders the average of $\Delta {\bf y}$ and $\Delta {\bf y}'$ meaningful.  See \algref{alg:CatStratPD} for more details.

\spd{} and \cspd{} both have theoretical worst-case time complexity of $O(n^2)$ for $n$ observations. For \spd{}, stratification costs $O(p n \,log n)$, computing $y$ deltas for all observations among the leaves has linear cost, and averaging slopes across unique $x_j$ ranges is on the order of $|unique({\bf X}_j)| \times n$ or $n^2$ when all ${\bf X}_j$ are unique in the worst case. \spd{} is, thus, $O(n^2)$ in the worst case.  \cspd{} also stratifies in $O(p n \,log n)$ and computes category deltas linearly in $n$ but must make multiple passes over the $|T|$ leaves to average all possible leaf category delta vectors.  In practice, three passes is the max we have seen (for high-cardinality variables), so we can assume the number of passes is some small constant to get a tighter bound. Averaging two vectors costs $|unique({\bf X}_j)|$, so each pass requires $|T| \times |unique({\bf X}_j)|$. The number of leaves is roughly $n$ / {\tt\small min\_samples\_leaf} and, worst-case, $|unique({\bf X}_j)|=n$, meaning that merging dominates \cspd{} complexity leading to $O(n^2)$.  Experiments show that our prototype is fast enough for practical use (see \secref{sec:experiments}).

% ------------------------------------------------------------------------------------------------------------------------------------
\section{Related work}\label{sec:related}

The mechanisms most closely related to \spd{} and \cspd{} are FPD, ICE, SHAP, and ALE, which all define partial dependence in terms of impact on estimated models, $\hat{f}$, rather than the unknown true function $f$. Let $x_S$ be the subset of features of interest where $S \subset F = \{1, 2, .., p\}$. \citet{PDP} defines the partial dependence as an expectation conditioned on the remaining variables:\vspace{-2mm}

\begin{equation}
{\it FPD}_S(x_S) = \Ex[\hat{f}(x_S,{\bf X}_{\slash S})],
\end{equation}\vspace{-3mm}

\noindent where the expectation can be estimated by $\frac{1}{n} \sum_{i=1}^n \hat{f}(x_S, x_{\slash S}^{(i)})$.
% ICE 
The Individual Conditional Expectation (ICE) plot \citep{ICE} estimates the partial dependence of the prediction $\hat{f}$ on $x_S$, or single variable $x_j$, across individual observations. ICE produces a curve from the fitted model over all values of $x_j$ while holding \xnj{} fixed: $\hat{f}^{(i)}_j = \widehat{f}(\{x_j^{(k)}\}_{k = 1}^n, x_{\slash j}^{(i)})$;  the FPD curve for  $x_j$ is the average over all $x_j$ ICE curves. The motivation for ICE is to identify variable interactions that average out in the FPD curve. 

% SHAP

The SHAP method from \citet{shap} has roots in {\em Shapley regression values} \citep{shapley-regression} and calculates the average marginal effect of adding $x_j$ to models, $\hat{f}_S$, trained on all possible subsets of features:
\vspace{-1mm}

\begin{equation}\label{eq:shap}
\phi_j(\hat{f},x_F) = \sum_{S \subseteq F \slash \{j\}}\
\frac{|S|!(|F|-|S|-1)!}{|F|!}\
 [ \hat{f}_{S \cup \{j\}}(x_{S \cup \{j\}}) - \hat{f}_S(x_S) ]
\end{equation}\vspace{-1mm}

To avoid training a combinatorial explosion of models with the various feature subsets, $\hat{f}_S(x_S)$, SHAP reuses a single model fitted to $(\bf X, y)$ by running simplified feature vectors into the model. As  \citet{manyshap} describes, there are many possible implementations for simplified vectors. One is to replace ``missing''  features with their expected value or some other baseline vector (BShap in \citealt{manyshap}). SHAP uses a more general approach (``interventional'' mode) that approximates $\hat{f}_S(x_S)$ with $\Ex[\hat{f}(x_{S},{\bf X}'_{\slash S}) | {\bf X'}_S = x_S]$ where ${\bf X}'$ is called the {\em background set} and users can pass in, for example, a single vector with ${\bf X}_{\slash S}$ column averages or even the entire training set, $\bf X$, which is what we will assume (and is called CES($\hat{D}$) in \citealt{manyshap} where $\hat{D}={\bf X}$). To further reduce computation costs, SHAP users typically explain a small subsample of the data set, but with potentially a commensurate reduction in the explanatory resolution of the underlying population. SHAP has model-type-dependent optimizations for linear regression, deep learning, and decision-tree based models.

For efficiency, SHAP approximates $\Ex[\hat{f}(x_{S},{\bf X}_{\slash S}) | {\bf X}_S = x_S]$ with $\Ex[\hat{f}(x_{S},{\bf X}_{\slash S})]$, which assumes feature independence. \citep{janzing2019feature} argues that ``{\em unconditional} [as implemented] rather than {\em conditional} [as defined] expectations provide the right notion of dropping features.''  But, using the unconditional expectation makes the inner difference of \eqref{eq:shap} a function of codependency-sensitive FPDs:

\vspace{-4mm}\begin{equation}
\begin{split}
\hat{f}_{S \cup \{j\}}(x_{S \cup \{j\}}) - \hat{f}_S(x_S) & = \Ex[\hat{f}(x_{S \cup \{j\}},{\bf X}_{\slash (S \cup \{j\})})] - \Ex[\hat{f}(x_{S},{\bf X}_{\slash S})]\\
 & = \text{\it FPD}_{S \cup \{j\}}({\bf x}) - \text{\it FPD}_{S}({\bf x})
\end{split}
\end{equation}\vspace{-2mm}

If the individual contributions are potentially biased, averaging the contributions of many such feature permutations might not lead to an accurate partial dependence.  Even if the conditional expectation is used, \citet{manyshap} points out that SHAP is sensitive to the sparsity of $\bf X$ because condition ${\bf X}_S = x_S$ will find few or no training records with the exact $x_S$ values of some input vector.

% ALE

The goal of ALE \citep{ALE} is to overcome the bias in previous model-based techniques arising from extrapolations of $\hat{f}$ far outside the support of the training data in the presence of codependent variables.   ALE  partitions range $[\text{min}({\bf X}_j) \,..\, \text{max}({\bf X}_j)]$ for variable $x_j$ into $K$ bins and estimates the ``uncentered main effect'' (Equation 15) at $x_j = z$ as the cumulative sum of the partial derivatives for all bins up to the bin containing $z$. ALE estimates the partial derivative of $\hat{f}$ at $x_j=z$ as $\Ex[\hat{f}(b_k, {\bf X}_{\slash j}) - \hat{f}(b_{k-1}, {\bf X}_{\slash j}) \, | \, x_j \in (b_{k-1},b_k]]$ for bin $b_k$ that contains $z$. They also extend ALE to two variables by partitioning feature space into $K^2$ rectangular bins and computing the  second-order finite difference of $\hat{f}$ with respect to the two variables for each bin. 

Another related technique that integrates over partial derivatives to measure $x_j$ effects is called Integrated Gradients (IG) from \citet{intgrad}. Given a single input vector, $\bf x$, to a deep learning classifier, IG integrates over the gradient of the model output function $\hat{f}$ at points along the path from a baseline vector, $\bf x'$, to $\bf x$. IG can be seen as computing the partial dependence of $\hat{f}$ at a single $\bf x$, but using multiple $\bf x$ vectors would yield an $x_j$ partial dependence curve (relative to a baseline $\bf x'$).

\spd{} is like a ``model-free'' version of ALE in that \spd{} also defines partial dependence as the cumulative sum of partial derivatives, but we estimate derivatives using response values directly rather than $\hat{f}$ predictions.  An advantage to estimating partial dependence via fitted models is that $\hat{f}$ removes variability from the potentially noisy response values, $\bf y$. However, in practice, this requires expertise to choose and tune an appropriate model for a data set.  The fact that different models can lead to meaningfully different curves for the same data can lead to misinterpretation.  Also, expertise is often required to distinguish between model artifacts and interesting visual phenomena arising from the data.

ALE partitions $x_j$ into bins then fixes \xnj{} as it shifts $x_j$ to bin edges to compute finite differences, as depicted in \figref{fig:partitioning}(b) for $p=2$. The wedges on the $x_1$ axis indicate the $x_1$ points of the computed partial dependence curve.  \spd{} partitions \xnj{} into regions of (hopefully) similar observations and computes finite differences between the average $y$ values at unique $x_j$ values in each region, as depicted in \figref{fig:partitioning}(a).  The leftmost two observations are ignored as there is no change in $x_1$ in that leaf. The shaded area illustrates that the partial derivative at any $x_j=z$ is the average of all derivatives spanning $z$ across \xnj{} regions.  \spd{} assumes all points within a region are identical in \xnj{}, effectively projecting points in \xnj{} space onto a hyperplane if they are not.  ALE shifts $x_j$ values in a small neighborhood and \spd{} depends on a suitable {\tt\small min\_samples\_leaf}  hyper parameter to prevent \xnj{} points in a regions from becoming too dissimilar.  \spd{} automatically generates more curve points in areas of high $x_j$ density, but ALE is more efficient.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width = \textwidth]{images/partitioning.pdf}\vspace{-3mm}
\caption{Comparison of \spd{}, \cspd{} and ALE for $p=2$ with continuous $x_2$ and continuous $x_1$ in (a), (b) and  categorical $x_1$ in (c), (d).  F=false and T=true. Vertical dashed lines are ALE bin edges and horizontal dashed lines are regions partitioned by a decision tree. Whereas ALE shifts $x_1$ observations to bin edges holding $x_2$ exactly constant (and asks for predictions at those points), \spd{} assumes $x_2$ values are the same and uses known $y$ values. The small wedges on $x_1$ axis indicate partial dependence curve points.  The shades of green and blue indicate $y$ values. The leftmost two observations in (a) and the lowermost observation in (c) are ignored as finite differences are not defined. There is no curve value for the rightmost $x_1$ value in (a) due to forward differencing.}
\label{fig:partitioning}
\end{center}
\end{figure}

Using decision trees for the purpose of partitioning feature space as \spd{} does was previously used by \citet{rfimp} to improve permutation importance for random forests by permuting $x_j$ only within the observations of a leaf. Earlier, \citet{RFunsup} defined a similarity measure between two observations according to how often they appear in the same leaf in a random forest.  Rather than partitioning  \xnj{} space like \spd, those techniques partitioned all of $x$ space.

Varying-coefficient models like those introduced in \citet{fan2008statistical} as well as local nonparametric methods like LOESS \citep{cleveland1979robust, cleveland1981lowess, cleveland1988locally} can model effect heterogeneities across the range of $x_j$; however, the partial dependence between $y$ and $x_j$ can be difficult to interpret with these models, and these approaches are not appropriate in high dimensional settings. 

% COMPARE

The model-based techniques under discussion treat boolean and label-encoded categorical variables (encoded as unique integers) as numerical variables, even though there is no defined order, as depicted in \figref{fig:partitioning}(d). ALE does, however, take advantage of the lack of order to choose an $x_j$ order that reduces ``extrapolation outside the data envelope'' by measuring the similarity of \Xnj{} sample values across $x_j$ categories.  Adjacent category integers, though, could still represent the most semantically different categories, so any shift of an observation's category to extrapolate is risky. Even the smallest possible extrapolation can conjure up nonsensical observations, such as pregnant males, as we demonstrate in \figref{fig:pregnant} below where FPD, SHAP, and ALE underestimate pregnancy's effect on body weight. (For boolean $x_j$, ALE behaves like FPD.)   See \cite{stopperm} for more on the dangers of permuting features. 

In contrast, \cspd{} uses a different algorithm for categoricals and computes differences between the average $y$ for all categories within the leaf to a random reference category; see \figref{fig:partitioning}(c). These leaf delta vectors are then merged across leaves to arrive at an overall delta vector relating the relative effect of each category on $y$.  One could argue that \spd{} also extrapolates because \xnj{} could include categorical variables and not all \xnj{} records would be identical. But, our approach only assumes \xnj{} values are similar and uses known training $y$ values for finite differences, rather than asking a model to make prediction for nonsensical records, which could be wildly inaccurate. Also, the decision tree would, by definition, likely partition \xnj{} space into regions that can be treated similarly, thus, grouping semantically similar categorical levels together.

\cut{
{\bf Comparison}

${\it FPD}_j(z) = \Ex[\hat{f}(x_{j}=z,{\bf X}_{\slash j})])$

SHAP for one $S$ is $\phi_j(\hat{f},{\bf x}) = \Ex[\hat{f}(x_{S \cup \{j\}},{\bf X}_{\slash (S \cup \{j\})})  | {\bf X}_{S \cup \{j\}} = x_{S \cup \{j\}}] - \Ex[\hat{f}(x_{S},{\bf X}_{\slash S})  | {\bf X}_S = x_S ]$ for all subsets $S \subset F$ for $F = \{1, 2, .., p\}$. When $S = F \slash \{j\}$, like ALE, it becomes $\Ex[\hat{f}(x_F)] - \Ex[\hat{f}(x_{F \slash \{j\}},{\bf X}_j)]$ or $\hat{f}({\bf x}) - \text{\it FPD}_{F \slash \{j\}}(x)$ if assume independence.

ALE at $x_j=z$, it is cumsum of $\Ex[\hat{f}(b_k, {\bf X}_{\slash j}) - \hat{f}(b_{k-1}, {\bf X}_{\slash j}) \, | \, x_j \in (b_{k-1},b_k]]$ for bin $b_k$ partitioning min..max for var $j$ into $K$ intervals.

StratPD at $x_j=z$ is $\Ex[ (y^{(i_L+1)} - y^{i_L})/(x_j^{(i_L+1)} - x_j^{(i_L)}) \, | \, z \in [x_j^{(i_L+1)} - x_j^{(i_L)}) \text{ and } L \in T]$

$\Ex[ \phi_j(\hat{f},x) | {\bf X}_j = z] = FPD_j(x) - \bar{y}$ if features independent. SHAP's implementation approximates $\hat{f}$ trained on just $S$ features, $\hat{f}_S(x_S)$, with $\Ex[\hat{f}(x_{S},{\bf X}_{\bar{S}})]$ by assuming independent features.
}

% ------------------------------------------------------------------------------------------------------------------------------------
\section{Numerical Study}\label{sec:experiments} 

In this section, we demonstrate experimentally that \spd{} and \cspd{} compute accurate partial dependence curves for synthetic data and plausible results for a real data set. Experiments also provide evidence that existing model-based techniques can provide meaningfully-biased curves. We begin by comparing the partial dependence curves from popular techniques on synthetic data with complex interactions.\footnote{All simulations in this section were run on a 4.0 Ghz 32G RAM machine running OS X 10.13.6 with SHAP 0.34, scikit-learn 0.21.3, XGBoost 0.90, TensorFlow 2.1.0, and Python 3.7.4; ALEPlot 1.1 and R 3.6.3. A single random seed was used across simulations for graph reproducibility purposes.}

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width = \textwidth]{images/interactions.pdf}\vspace{-2mm}
\caption{Partial dependence plots of $n=2000$ data generated from noiseless $y = x_1^2 + x_1 x_2 + 5 x_1 sin(3 x_2) + 10$ where $x_1,x_2,x_3 \sim U(0,10)$ and $x_3$ does not affect $y$. For ALE, SHAP, and FDP, a random forest model is fit with 10 trees trained on all data ($R^2=0.997$). SHAP used all $\bf X$ as background data and ALE used $K=300$. The curves were generated from same 2000 data points that the model was trained on.}
\label{fig:interactions}
\end{center}
\end{figure}

\figref{fig:interactions} illustrates that FPD, SHAP, ALE, and \spd{} all suitably isolate the effect of independent individual variables on the response for noiseless data generated via: $y = x_1^2 + x_1 x_2 + 5 x_1 sin(3 x_2) + 10$ for $x_1,x_2,x_3 \sim U(0,10)$ where $x_3$ does not affect $y$. The shapes of the curves for all techniques look similar except that \spd{} starts all curves at $y=0$ (as could the others). SHAP's curves have the advantage that they indicate the presence of variable interactions. To our eye, \spd's curves are smoothest despite not having access to model predictions.



Models have a tendency to smooth out noise and a legitimate concern is that, without the benefit of a model, \spd{} could be adversely affected. \figref{fig:noise} demonstrates \spd{} curves for noisy quadratics generated from $y = x_1^2 + x_2 + 10 + \epsilon$ where $\epsilon \sim N(0,\sigma)$ $\epsilon$ and, at $\sigma=2$, 95\% of the noise falls within [0,4] (since $2\sigma = 4$), meaning that the signal-to-noise ratio is at best 1-to-1 for $x_1^2$ in $[-2,2]$. For zero-centered Gaussian noise and this data set, \spd{} appears resilient, though $\sigma=2$ does show considerable variation across runs.  The superfluous noise variable $x_3$ in \figref{fig:interactions} also did not confuse \spd.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[width = \textwidth]{images/noise.pdf}\vspace{-2mm}\caption{The effective noise on \spd{} for $y = x_1^2 + x_1 + 10 + \epsilon$ where $x_1,x_2 \sim U(-2,2)$, $\epsilon \sim N(0,\sigma)$ with $\sigma \in \{0,0.5,1,2\}$. The 95\% interval for amplitude of the noise in (d) for $\sigma=2$ is the same as the signal.}
\label{fig:noise}
\end{center}
\end{figure}

Turning to categorical variables, \figref{fig:statetemp} presents partial dependence curves for FPD, ALE, and \cspd{} derived from a noisy synthetic weather data set, where temperature varies in sinusoidal fashion over the year and with different baseline temperatures per state. (The vertical ``smear'' in the FPD plot shows the complete sine waves but from the side, edge on.)  Variable {\tt\small state} is independent and all plots identify the baseline temperature per state correctly.

\begin{figure}[!htbp]
\begin{center}
\includegraphics[scale=0.62]{images/dayofyear_vs_temp.pdf}~~
\includegraphics[scale=0.62]{images/state_vs_temp_pdp.pdf}~~
\includegraphics[scale=0.62]{images/state_ale.pdf}~~
\includegraphics[scale=0.62]{images/state_vs_temp_stratpd.pdf}\vspace{-1mm}
\caption{$y = base[x_{\it state}] + 10 sin(\frac{2\pi}{365}x_{\it dayofyear}+\pi) + \epsilon$ where $\epsilon \sim N(\mu=0, \sigma=4)$. The {\em base} temperature per state is $\{{\it AZ}=90, {\it CA}=70, {\it CO}=40, {\it NV}=80, {\it WA}=60\}$. Sinusoids in (a) are the average of three years' temperature data.}
\label{fig:statetemp}
\end{center}
\end{figure}

The primary goal of the stratification approach proposed in this paper is to obtain accurate partial dependence curves in the presence of codependent variables. To test \spd{}/\cspd{} and discover potential bias in existing techniques, we synthesized a body weight data set generated by the following equation with nontrivial codependencies between variables: 
\vspace{-1mm}

\begin{equation}\label{eq:weight}
{
\begin{array}{rll}
y & = &120 + 10(x_{height} - min(x_{height})) + 40x_{pregnant} - 1.5x_{education}\\[2pt]
\vspace{-10pt}\\[2pt]
\multicolumn{2}{r}{\text{where}} & x_{sex} \sim Bernoulli(\{M,F\}, p=0.5)\\[5pt]
                    & & x_{pregnant} = \begin{cases}
                                               Bernoulli(\{0,1\},p=0.5) & \text{ if } x_{sex} = F\\
                                               0 & \text{ if } x_{sex}=M\\
                                               \end{cases}\\[15pt]
                    & & x_{height} = \begin{cases}
                                               5*12+5+ \epsilon & \text{ if } x_{sex}=F,~ \epsilon \sim U(-4.5,5)\\	
                                               5*12+8 + \epsilon & \text{ if } x_{sex}=M,~ \epsilon \sim U(-7,8)\\
                                               \end{cases}\\[15pt]
                    & & x_{education} = \begin{cases}
                                               12 + \epsilon & \text{ if } x_{sex}=F,~ \epsilon \sim U(0,8)\\	
                                               10 + \epsilon & \text{ if } x_{sex}=M,~ \epsilon \sim U(0,8)\\
                                               \end{cases}
\end{array}
}
\end{equation}\vspace{1mm}

\noindent The partial derivative of $y$ with respect to $x_{height}$ is 10 (holding all other variables constant), so the optimal partial dependence curve is a line with slope 10. \figref{fig:heightweight} illustrates the curves for the techniques under consideration, with ALE and \spd{} giving the sharpest representation of the linear relationship. (\spd's curve is drawn on top of the SHAP plots using the righthand scale.) The FPD and both SHAP plots also suggest a linear relationship, albeit with a little less precision. The ICE curves in \figref{fig:heightweight}(a) and ``fuzzy'' SHAP curves have the advantage that they alert users to variable  dependencies or interaction terms.  On the other hand, the kink in the partial dependence curve and other visual phenomena could confuse less experienced machine learning practitioners and certainly analysts and researchers in other fields (our primary target communities).

\begin{figure}[!htbp]
\begin{center}
\includegraphics[scale=0.66]{images/height_vs_weight_pdp.pdf}~\includegraphics[scale=0.66]{images/weight_tree_path_dependent_shap.pdf}
\includegraphics[scale=0.66]{images/weight_interventional_shap.pdf}~
\includegraphics[scale=0.66]{images/height_ale.pdf}\vspace{-2mm}
\caption{Partial dependence plots of response body weight on feature $x_{height}$ using $n$=2000 synthetic observations from \eqref{eq:weight}. FPD and SHAP have ``kinks'' at the maximum female height. SHAP defines feature importance as the average SHAP value magnitude, which overemphasizes importance for heights below 70 inches here. The male/female ratio is 50/50, half of the women are pregnant, and pregnancy contributes 40 pounds. All partial dependence techniques interrogated an RF with 200 trees, tuned via 5-fold cross validation grid search (OOB $R^2$ 0.999). SHAP explained all 2000 samples and the SHAP interventional case used 100 observations as background data. ALE used $K=300$.}
\label{fig:heightweight}
\end{center}
\end{figure}

Even for experts, explaining this behavior requires some thought, and one must distinguish between model artifacts and interesting phenomena. The discontinuity at the maximum female height location arises partially from the model having trouble extrapolating for extremely tall pregnant women. Consider one of the upper ICE lines in \figref{fig:heightweight}(a) for a pregnant woman. As the ICE line slides $x_{height}$ above the maximum height for a woman, the model leaves the support of the training data and predicts a {\em lower} weight as height increases (there are no pregnant men in the training data). ALE's curve is straight because it focuses on local effects, demonstrating that the lack of sharp slope=10 lines for FPD and SHAP cannot be attributed simply to a poor choice of model since they all use the same model.  

\begin{figure}[!htbp]
\begin{center}
\includegraphics[scale=0.8]{images/pregnant_vs_weight_pdp.pdf}~~
\includegraphics[scale=0.8]{images/pregnant_vs_weight_shap.pdf}~~
\includegraphics[scale=0.8]{images/pregnant_2_ale.pdf}~~
\includegraphics[scale=0.8]{images/pregnant_vs_weight_stratpd.pdf}\vspace{-2mm}
\caption{Partial dependence bar charts for boolean $x_{pregnant}$ with the same model from \figref{fig:heightweight}. Only \cspd{} gets the correct 40 pound contribution per \eqref{eq:weight}.}
\label{fig:pregnant}
\end{center}
\end{figure}

Also, SHAP defines $x_j$ feature importance as the average magnitude of the $x_j$ SHAP values, which introduces a paradox.  The spread of the SHAP values alerts users to variable interactions, but allows contributions from other variables to leak in, thus, potentially leading to less precise estimates of $x_{height}$'s importance. The average SHAP magnitude skews upward, in this case, because of the contributions from pregnant women.

It is conceivable that a more sophisticated model (in terms of extrapolation) could sharpen the FPD and SHAP curves for $x_{height}$. There is a difference, however, between extrapolating to a meaningful but unsupported vector and making predictions for nonsensical vectors arising from variable codependencies.  Techniques that rely on such predictions make the implicit assumption of variable independence, introducing the potential for bias. Consider \figref{fig:pregnant} that presents the partial dependence results for categorical variable $x_{pregnant}$ (same data set). The weight gain from pregnancy is 40 pounds per \eqref{eq:weight}, but only \cspd{} identifies that exact relationship; FPD, SHAP, and ALE show a gain of 30 pounds. 

\cspd{} stratifies persons with the same or similar sex, education, and height into groups and then examines the relationship between $x_{pregnant}$ and $y$. If a group contains both pregnant and nonpregnant females, the difference in weight will be 40 pounds in this noiseless data set (if we assume identical \xnj).  FPD and ALE rely on computations that require fitted models to conjure up predictions for nonsensical records representing pregnant males (e.g., $\hat{f}(x_j = pregnant, {\bf X}_{\slash j})$). Not even a human knows how to estimate the weight of a pregnant male. SHAP, per its definition, does not require such predictions, but in practice for efficiency reasons, SHAP approximates $\Ex[\hat{f}(x_{j} = pregnant,{\bf X}_{\slash j}) | {\bf X}_j = x_j]$ with $\Ex[\hat{f}(x_{j} = pregnant,{\bf X}_{\slash j})]$, which does not restrict pregnancy to females. As discussed above, there are advantages to all of these model-based techniques, but this example demonstrates there is potential for partial dependence bias.

\cut{
pregnant female at max range [233.33467678]
pregnant female in male height range [228.71359869]
nonpregnant female in male height range [209.88006493]
male in male height range [219.46772083]
}


\cut{
$1/n*\sum_{i=1}^n f(x_j = pregnant, x_{i, \bar{j}}) - f(x_j = not pregnant, x_{i, \bar{j}})$

\noindent where ${x_{i, \bar{j}}: i=1,2,?,n}$ are the n training observations of $x_{\bar{j}}$
}

\begin{figure}[!htbp]
\begin{center}
\includegraphics[scale=0.41]{images/bulldozer_YearMade_pdp.pdf}~~
\includegraphics[scale=0.41]{images/bulldozer_YearMade_shap.pdf}~~
\includegraphics[scale=0.41]{images/bulldozer_YearMade_ale.pdf}~~
\includegraphics[scale=0.41]{images/bulldozer_YearMade_stratpd.pdf}\vspace{-2mm}
\caption{Partial dependence curves of bulldozer {\tt YearMade} versus {\tt SalePrice} for FPD/ SHAP, ALE, and \spd. $n$=20,000 observations drawn from \textasciitilde{}362k. SHAP interrogated a random forest (OOB $R^2=0.85$) to explain 1000 training observations with 100 observations as background data.  ALE used $K=300$. Hyper parameters were tuned using 5-fold cross validation grid search over several hyper parameters.}
\label{fig:yearmade}
\end{center}
\end{figure}


The stratification approach also gives plausible results for real data sets, such as the bulldozer auction data from \citet{bulldozer}. (The data set has 52 columns that describe a bulldozer and the price it received during auction; the goal of the competition was to predict to the auction price of a new bulldozer.) \figref{fig:yearmade} shows the partial dependence curves for the same set of techniques as before on feature {\tt\small YearMade}, chosen as a representative because it is very predictive of sale price. The shape and magnitude of the FPD, SHAP, ALE, and \spd{} curves are similar, indicating that older bulldozers are worth less at auction, which is plausible. The \spd{} curve shows 10 bootstrapped trials where the heavy black dots represent the partial dependence curve and the other colored curves describe the variability. 

\begin{figure}[!htbp]
\begin{center}
\includegraphics[scale=0.41]{images/bulldozer_ProductSize_pdp.pdf}~~
\includegraphics[scale=0.41]{images/bulldozer_ProductSize_shap.pdf}~~
\includegraphics[scale=0.41]{images/bulldozer_ProductSize_ale.pdf}~~
\includegraphics[scale=0.41]{images/bulldozer_ProductSize_stratpd.pdf}
\caption{Partial dependence curves of bulldozer {\tt ProductSize} versus {\tt SalePrice} for FPD/ICE, SHAP, ALE, and \spd. $n$=20,000 observations drawn from \textasciitilde{}362k. Same model, setup as in \figref{fig:yearmade}.  The \spd{} dots are the average of 10 bootstrapped trials and the partial dependence for {\tt ProductSize} is missing because a forward difference is unavailable at the right edge.}
\label{fig:ProductSize}
\end{center}
\end{figure}



As a second example, consider the curves for feature {\tt\small ProductSize} in \figref{fig:ProductSize}. All plots have roughly the same shape but the \spd{} plot lacks a dot for {\tt\small ProductSize}=5 because  forward differences are unavailable at the right edge.  (We anticipate switching switching to a central difference to avoid this issue with low-cardinality discrete variables.) It also appears that \spd{}  considers {\tt\small ProductSize}'s 3 and 4 to be worth less than suggested by the other techniques, though \spd{}'s might be in line with the average SHAP plot values.



And, finally, an important consideration for any tool is performance, so we plotted execution time versus data size (up to 30,000 observations) for three real Kaggle data sets: rent, bulldozer, and flight arrival delays \citep{flights}. \figref{fig:timing} shows growth curves for 40 numerical variables and 11 categorical variables grouped by type of variable.  For these data sets, \spd{} takes 1.2s or less to process 30,000 records for any  numerical $x_j$, despite the potential for quadratic cost. \cspd{} typically processes categorical variables in less than 2s but takes 13s for the high-cardinality categorical {\tt\small ModelID} of bulldozer (which looks mildly quadratic).  These elapsed times for our prototype show it to be practical and competitive with FPD/ICE, SHAP, and ALE.  If the cost to train a model using (cross validated) grid search for hyper parameter tuning is included, \spd{} and \cspd{} outperform these existing techniques (as training and tuning is often measured in minutes).
\begin{figure}[!htbp]
\begin{center}
\includegraphics[width = .65\textwidth]{images/timing.pdf}\vspace{-3mm}
\caption{Time to compute partial dependence curve for up to 30,000 observations for 40 numerical and 11 categorical variables. $p=20$, bulldozer $p=14$, and flight $p=17$. The Numba just-in-time compiler is used to improve performance, but timing does not include compilation; users do experience this ``warm-up'' time.}
\label{fig:timing}
\end{center}
\end{figure}

\cut{
flight shape (5714008, 17), 6 cats
rent shape (49352, 20), no cats
bulldozer shape (362781, 14) records, 5 cats
}

% ------------------------------------------------------------------------------------------------------------------------------------
\section{Discussion and future work}

In this paper, we contribute a method for computing partial dependence curves, for both numerical and categorical explanatory variables, that does not use  predictions from a fitted model.   Working directly from the data  makes partial dependences accessible to business analysts and scientists not qualified to choose, tune, and assess machine learning models.  For experts, it can  provide hints about the relationships in the data to help guide their choice of model. Our experiments show that \spd{} and \cspd{} are fast enough for practical use and correctly identify partial dependences for synthetic data and give plausible curves on real data sets. \spd{} relies on two important hyper parameters (with broadly applicable defaults) but model-based techniques should include the hyper parameters of the required fitted model for a fair comparison.  Our goal here is not to  argue that model-based techniques are not useful. Rather, we are pointing out potential issues and hoping to open a new line of nonparametric inquiry that experiments have shown to be applicable in situations and accurate in cases where model-based techniques are not. In the future, we aim to generalize this approach to classifiers and to extend the technique to two or more variables.

\section*{Declaration of Interests}
\noindent The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

\section*{CRediT Author Statement}
\noindent {\bf Terence Parr}: Conceptualization, Investigation, Methodology, Software, Writing- Original draft preparation. {\bf James D. Wilson}: Conceptualization, Investigation, Validation, Writing- Reviewing and Editing.
\section*{Acknowledgments}
\noindent JDW acknowledges partial support from the National Science Foundation, grant NSF DMS-1830547.

\bibliographystyle{elsarticle-num-names} 
\bibliography{stratpd}

\appendix

\section{Pseudocode for \spd{} and \cspd}

\SetAlgoNoEnd%
\setlength{\algomargin}{5pt}
\begin{algorithm}[H]
\SetAlgoLined
\DontPrintSemicolon
\SetAlgorithmName{Algorithm}{List of Algorithms}
\SetAlgoSkip{}
\TitleOfAlgo{{\em StratPD}({\bf X}, {\bf y}, $j$, {\it min\_samples\_leaf}, {\it min\_slopes\_per\_x})}
$T$ := Decision tree regressor fit to (\Xnj{}, $\bf y$) with hyper parameter: ${\it min\_samples\_leaf}$\;
\For{each leaf $L \in T$}{
        ${\bf x}_L := \{x_j^{(i)}\}_{i \in L}$, the unique and ordered $x_j$ in $L$%\Comment*{\it Get leaf observations}
        $\bar{\bf y}_L^{(k)} := \Ex[ y^{(i)} \,|\, (x_j^{(i)}=x_L^{(k)},  y^{(i)})] \text{ for } k = 1..|{\bf x}_L|, i \in L$%\Comment*{\it Compute $\bar{y}$ per unique $x_j$ value}
        $\delta_L^{(k)} = \frac{\bar{\bf y}_L^{(k+1)} - \bar{\bf y}_L^{(k)}}{{\bf x}_L^{(k+1)} - {\bf x}_L^{(k)}}$%\Comment*{\it Discrete forward difference between adjacent unique $x_j$ values}
	Add tuples ${({\bf x}_L^{(k)}, {\bf x}_L^{(k+1)},~ \delta_L^{(k)})}$  to list ${\bf D}$ for $k = 1..|{\bf x}_L|-1$\\
}
${\bf ux} := \{x_j^{(i)}\}_{i \in 1..n}$, the unique and ordered $x_j$ in ${\bf X}_j$\\
Let $\bf c$ and $\delta$ be vectors of length $|\bf ux|$\\
\For(\hfill$\triangleright$\ {\it\textcolor{gray}{\small Count slopes and compute average slope per unique $x_j$ value}}){each $x \in {\bf ux}$}{
	${\bf slopes}_x$ := [$slope$ for $(a,b,slope) \in \bf D$ if $x \ge a$ and $x <b$]\\
	${\bf c}_x$ := $|{\bf slopes}_x|$%\Comment*{\it Count number of slopes used to estimate overall slope at $x$}
	${\delta}_x$ := $\overline{\bf slopes}_x$%\Comment*{\it Slope estimate at $x$ is average of slopes whose range overlaps $x$}
}
${\bf \delta}$ := ${\bf \delta}[{\bf c} \ge min\_slopes\_per\_x]$%\Comment*{\it Drop missing slopes and those computed with too few}
${\bf ux}$ := ${\bf ux}[{\bf c} \ge min\_slopes\_per\_x]$%\Comment*{\it Drop $x_j$ values without sufficient estimates}
${\bf pd}_x$ := ${\bf ux}^{(k+1)} - {\bf ux}^{(k)}$ for $k = 1..|{\bf ux}|-1$\\
${\bf pd}_y$ := [0] + cumulative\_sum($\delta \times {\bf pd}_x$)~~~%\Comment*{\it integrate, inserting 0 for leftmost $x_j$}
\Return{${\bf pd}_x, {\bf pd}_y$}
\label{alg:StratPD}
\end{algorithm}

\vspace{10mm}

\setlength{\algomargin}{5pt}
\begin{algorithm}[H]
\SetAlgoLined
\DontPrintSemicolon
\SetAlgorithmName{Algorithm}{List of Algorithms}
\SetAlgoSkip{}
\TitleOfAlgo{{\em CatStratPD}(${\bf X}, {\bf y}, j, {\it min\_samples\_leaf}$)}

\cut{
\KwOut{$\begin{array}[t]{l}
\Delta^{(k)} = \text{category } k \text{'s effect on } y \text{ where } mean(\Delta^{(k)})=0\\
n^{(k)} = \text{number of supported observations per category $k$}\\
\end{array}$
}
}
$n_{cats} := |\{x_j^{(i)}\}_{i \in 1..n}|$, $n_{leaves} := |T|$\\
$T$ := Decision tree regressor fit to (\Xnj{}, $\bf y$) with hyper parameter: ${\it min\_samples\_leaf}$\;
Let $\Delta Y$ be a $n_{cats} \times n_{leaves}$ matrix whose columns, $\Delta Y_L$, are vectors of leaf category deltas\\
Let $C$ be a $n_{cats} \times n_{leaves}$ matrix whose columns, $C_L$, are vectors for leaf category counts\\
\For(\hfill$\triangleright$\ {\it\textcolor{gray}{\small Get average $y$ delta relative to random ref category for obs. in leaves}}){each leaf $L \in T$}{
        ${\bf x}_L := \{x_j^{(i)}\}_{i \in L}$, the unique $x_j$ categories in $L$%\Comment*{\it Get leaf observations}
        $\bar{\bf y}_L^{(k)} := \Ex[ y^{(i)} \,|\, (x_j^{(i)}=x_L^{(k)},  y^{(i)})] \text{ for } k = 1..|{\bf x}_L|, i \in L$%\Comment*{\it Compute $\bar{y}$ per unique $x_j$ category level}
        $C_L^{(k)} := |i \in L : x_j^{(i)}={\bf x}_L^{(k)}|$%\Comment*{\it Count occurrences of each category}
	${\it refcat}$ := random category chosen from ${\bf x}_L$\\
	$\Delta {Y}_L$ = $\bar{\bf y}_L - \bar{\bf y}_L^{\it refcat}$\\
}
$\Delta {\bf y}$, {\bf c} := $\Delta {Y}_1$, $C_1$%\Comment*{\it $\Delta {\bf y}$ is running average vector mapping category to average $y$ delta}
$completed$ := $\{L_1\}$; $work$ := $\{L_2 .. L_{n_{leaves}}\}$; %\Comment*{\it sets of leaves}
\While(\hfill$\triangleright$\ {\it\textcolor{gray}{\small 2 passes is typical to merge all $\Delta {Y}_L$ into $\Delta {\bf y}$}}){$|work| > 0$ and $|completed|>0$}{
    completed := $\emptyset$\\
    \For{each leaf $L$ in work}{
        Let $common$ := categories in common between $\Delta {Y}_L$ and $\Delta \bf y$\\
        \If{$common \ne \emptyset$}{
            ${\it completed}$ := ${\it completed} \cup \{L\}$\\
            $cat$ := random category in $common$\\
            $\Delta Y_L$ := $\Delta {Y}_L- \Delta {Y}_{L}^{(cat)} + \Delta {\bf y}^{(cat)}$%\Comment*{\it Adjust so $\Delta {Y}_{L}^{(cat)}=0$ then add corresponding $\Delta {\bf y}^{(cat)}$ value}
            $\Delta {\bf y}$ := $({\bf c} \times \Delta {\bf y} + C_L \times \Delta Y_L)/({\bf c} + C_L)$ where $z+NaN$={\it z}%\Comment*{\it weighted average}
            ${\bf c} := {\bf c} + C_L$%\Comment*{\it update running weight}
        }
    }
    $work := work \slash {\it completed}$\\
}
\Return{$\Delta {\bf y}$}\\
\label{alg:CatStratPD}
\end{algorithm}

\end{document}
