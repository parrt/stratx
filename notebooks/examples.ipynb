{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Mapping, List, Tuple\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_boston, load_iris, load_wine, load_digits, \\\n",
    "    load_breast_cancer, load_diabetes, fetch_mldata\n",
    "from  matplotlib.collections import LineCollection\n",
    "import time\n",
    "from pandas.api.types import is_string_dtype, is_object_dtype, is_categorical_dtype, is_bool_dtype\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
    "from pdpbox import pdp\n",
    "from rfpimp import *\n",
    "from scipy.integrate import cumtrapz\n",
    "from dtreeviz.trees import *\n",
    "\n",
    "\n",
    "def df_string_to_cat(df:pd.DataFrame) -> dict:\n",
    "    catencoders = {}\n",
    "    for colname in df.columns:\n",
    "        if is_string_dtype(df[colname]) or is_object_dtype(df[colname]):\n",
    "            df[colname] = df[colname].astype('category').cat.as_ordered()\n",
    "            catencoders[colname] = df[colname].cat.categories\n",
    "    return catencoders\n",
    "\n",
    "\n",
    "def df_cat_to_catcode(df):\n",
    "    for col in df.columns:\n",
    "        if is_categorical_dtype(df[col]):\n",
    "            df[col] = df[col].cat.codes + 1\n",
    "\n",
    "\n",
    "def toy_x1_times_x2_data(n=100):\n",
    "    df = pd.DataFrame()\n",
    "    i = np.linspace(0, 10, num=n)\n",
    "    df['x1'] = np.random.uniform(0, 1, size=n)\n",
    "    df['x2'] = np.random.uniform(0, 1, size=n)\n",
    "    df['y'] = df['x1'] * df['x2']# + df['x1'] + df['x2']\n",
    "    return df, f\"y = x1x2\\nx1, x2 in U(0,1)\", (0,.6)\n",
    "\n",
    "\n",
    "def toy_2x1_times_3x2_data(n=100):\n",
    "    df = pd.DataFrame()\n",
    "    i = np.linspace(0, 10, num=n)\n",
    "    df['x1'] = np.random.uniform(0, 1, size=n)\n",
    "    df['x2'] = np.random.uniform(0, 1, size=n)\n",
    "    df['y'] = 2*df['x1'] * 3*df['x2']# + df['x1'] + df['x2']\n",
    "    return df, f\"y = 2x1 * 3x2\\nx1, x2 in U(0,1)\", (0,4)\n",
    "\n",
    "\n",
    "def toy_weight_data(n):\n",
    "    df = pd.DataFrame()\n",
    "    nmen = n//2\n",
    "    nwomen = n//2\n",
    "    df['ID'] = range(100,100+n)\n",
    "    df['sex'] = ['M']*nmen + ['F']*nwomen\n",
    "    df.loc[df['sex']=='F','pregnant'] = np.random.randint(0,2,size=(nwomen,))\n",
    "    df.loc[df['sex']=='M','pregnant'] = 0\n",
    "    df.loc[df['sex']=='M','height'] = 5*12+8 + np.random.uniform(-7, +8, size=(nmen,))\n",
    "    df.loc[df['sex']=='F','height'] = 5*12+5 + np.random.uniform(-4.5, +5, size=(nwomen,))\n",
    "    df.loc[df['sex']=='M','education'] = 10 + np.random.randint(0,8,size=nmen)\n",
    "    df.loc[df['sex']=='F','education'] = 12 + np.random.randint(0,8,size=nwomen)\n",
    "    df['weight'] = 120 \\\n",
    "                   + (df['height']-df['height'].min()) * 10 \\\n",
    "                   + df['pregnant']*10 \\\n",
    "                   - df['education']*1.2\n",
    "    df['pregnant'] = df['pregnant'].astype(bool)\n",
    "    df['education'] = df['education'].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def toy_weather_data():\n",
    "    def temp(x): return np.sin((x+365/2)*(2*np.pi)/365)\n",
    "    def noise(state): return np.random.normal(-5, 5, sum(df['state'] == state))\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['dayofyear'] = range(1,365+1)\n",
    "    df['state'] = np.random.choice(['CA','CO','AZ','WA'], len(df))\n",
    "    df['temperature'] = temp(df['dayofyear'])\n",
    "    df.loc[df['state']=='CA','temperature'] = 70 + df.loc[df['state']=='CA','temperature'] * noise('CA')\n",
    "    df.loc[df['state']=='CO','temperature'] = 40 + df.loc[df['state']=='CO','temperature'] * noise('CO')\n",
    "    df.loc[df['state']=='AZ','temperature'] = 90 + df.loc[df['state']=='AZ','temperature'] * noise('AZ')\n",
    "    df.loc[df['state']=='WA','temperature'] = 60 + df.loc[df['state']=='WA','temperature'] * noise('WA')\n",
    "    return df\n",
    "\n",
    "\n",
    "def scramble(X : np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    From Breiman: https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
    "    \"...the first coordinate is sampled from the N values {x(1,n)}. The second\n",
    "    coordinate is sampled independently from the N values {x(2,n)}, and so forth.\"\n",
    "    \"\"\"\n",
    "    X_rand = X.copy()\n",
    "    ncols = X.shape[1]\n",
    "    for col in range(ncols):\n",
    "        X_rand[:,col] = np.random.choice(np.unique(X[:,col]), len(X), replace=True)\n",
    "    return X_rand\n",
    "\n",
    "\n",
    "def df_scramble(X : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    From Breiman: https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
    "    \"...the first coordinate is sampled from the N values {x(1,n)}. The second\n",
    "    coordinate is sampled independently from the N values {x(2,n)}, and so forth.\"\n",
    "    \"\"\"\n",
    "    X_rand = X.copy()\n",
    "    for colname in X:\n",
    "        X_rand[colname] = np.random.choice(X[colname].unique(), len(X), replace=True)\n",
    "    return X_rand\n",
    "\n",
    "\n",
    "def conjure_twoclass(X):\n",
    "    \"\"\"\n",
    "    Make new data set 2x as big with X and scrambled version of it that\n",
    "    destroys structure between features. Old is class 0, scrambled is class 1.\n",
    "    \"\"\"\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X_rand = df_scramble(X)\n",
    "        X_synth = pd.concat([X, X_rand], axis=0)\n",
    "    else:\n",
    "        X_rand = scramble(X)\n",
    "        X_synth = np.concatenate([X, X_rand], axis=0)\n",
    "    y_synth = np.concatenate([np.zeros(len(X)),\n",
    "                              np.ones(len(X_rand))], axis=0)\n",
    "    return X_synth, y_synth\n",
    "\n",
    "\n",
    "def ICE_predict(model, X:pd.DataFrame, colname:str, targetname=\"target\", numx=50, nlines=None):\n",
    "    \"\"\"\n",
    "    Return dataframe with one row per observation in X and one column\n",
    "    per unique value of column identified by colname.\n",
    "    Row 0 is actually the sorted unique X[colname] values used to get predictions.\n",
    "    It's handy to have so we don't have to pass X around to other methods.\n",
    "    Points in a single ICE line are the unique values of colname zipped\n",
    "    with one row of returned dataframe. E.g.,\n",
    "\n",
    "    \tpredicted weight          predicted weight         ...\n",
    "    \theight=62.3638789416112\t  height=62.78667197542318 ...\n",
    "    0\t62.786672\t              70.595222                ... unique X[colname] values\n",
    "    1\t109.270644\t              161.270843               ...\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    save = X[colname].copy()\n",
    "    if nlines is not None:\n",
    "        X = X.sample(nlines, replace=False)\n",
    "    if numx is not None:\n",
    "        linex = np.linspace(np.min(X[colname]), np.max(X[colname]), numx)\n",
    "    else:\n",
    "        linex = sorted(X[colname].unique())\n",
    "    lines = np.zeros(shape=(len(X) + 1, len(linex)))\n",
    "    lines[0, :] = linex\n",
    "    i = 0\n",
    "    for v in linex:\n",
    "        #         print(f\"{colname}.{v}\")\n",
    "        X[colname] = v\n",
    "        y_pred = model.predict(X)\n",
    "        lines[1:, i] = y_pred\n",
    "        i += 1\n",
    "    X[colname] = save\n",
    "    columns = [f\"predicted {targetname}\\n{colname}={str(v)}\"\n",
    "               for v in linex]\n",
    "    df = pd.DataFrame(lines, columns=columns)\n",
    "    stop = time.time()\n",
    "    print(f\"ICE_predict {stop - start:.3f}s\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def ICE_lines(ice:np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return a 3D array of 2D matrices holding X coordinates in col 0 and\n",
    "    Y coordinates in col 1. result[0] is first 2D matrix of [X,Y] points\n",
    "    in a single ICE line for single sample. Shape of result is:\n",
    "    (nsamples,nuniquevalues,2)\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    linex = ice.iloc[0,:] # get unique x values from first row\n",
    "    # If needed, apply_along_axis() is faster than the loop\n",
    "    # def getline(liney): return np.array(list(zip(linex, liney)))\n",
    "    # lines = np.apply_along_axis(getline, axis=1, arr=ice.iloc[1:])\n",
    "    lines = []\n",
    "    for i in range(1,len(ice)): # ignore first row\n",
    "        liney = ice.iloc[i].values\n",
    "        line = np.array(list(zip(linex, liney)))\n",
    "        lines.append(line)\n",
    "    stop = time.time()\n",
    "    # print(f\"ICE_lines {stop - start:.3f}s\")\n",
    "    return np.array(lines)\n",
    "\n",
    "\n",
    "def plot_ICE(ice, colname, targetname=\"target\", cats=None, ax=None, linewidth=.7, color='#9CD1E3',\n",
    "             alpha=.1, title=None, yrange=None, pdp=True, pdp_linewidth=1, pdp_alpha=1,\n",
    "             pdp_color='black'):\n",
    "    start = time.time()\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "\n",
    "    avg_y = np.mean(ice[1:], axis=0)\n",
    "\n",
    "    min_pdp_y = avg_y[0] if cats is None else 0\n",
    "    # if 0 is in x feature and not on left/right edge, get y at 0\n",
    "    # and shift so that is x,y 0 point.\n",
    "    linex = ice.iloc[0,:] # get unique x values from first row\n",
    "    nx = len(linex)\n",
    "    if linex[int(nx*0.05)]<0 or linex[-int(nx*0.05)]>0:\n",
    "        closest_x_to_0 = np.abs(linex - 0.0).argmin()\n",
    "        min_pdp_y = avg_y[closest_x_to_0]\n",
    "\n",
    "    lines = ICE_lines(ice)\n",
    "    lines[:,:,1] = lines[:,:,1] - min_pdp_y\n",
    "    # lines[:,:,0] scans all lines, all points in a line, and gets x column\n",
    "    minx, maxx = np.min(lines[:,:,0]), np.max(lines[:,:,0])\n",
    "    miny, maxy = np.min(lines[:,:,1]), np.max(lines[:,:,1])\n",
    "    if yrange is not None:\n",
    "        ax.set_ylim(*yrange)\n",
    "    else:\n",
    "        ax.set_ylim(miny, maxy)\n",
    "    ax.set_xlabel(colname)\n",
    "    ax.set_ylabel(targetname)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    lines = LineCollection(lines, linewidth=linewidth, alpha=alpha, color=color)\n",
    "    ax.add_collection(lines)\n",
    "\n",
    "    if cats is not None:\n",
    "        if True in cats or False in cats:\n",
    "            ax.set_xticks(range(0, 1+1))\n",
    "            ax.set_xticklabels(cats)\n",
    "            ax.set_xlim(0, 1)\n",
    "        else:\n",
    "            ncats = len(cats)\n",
    "            ax.set_xticks(range(1, ncats+1))\n",
    "            ax.set_xticklabels(cats)\n",
    "            ax.set_xlim(1, ncats)\n",
    "    else:\n",
    "        ax.set_xlim(minx, maxx)\n",
    "\n",
    "    ax.set_title(f\"Partial dependence of {colname} on {targetname}\")\n",
    "\n",
    "    if pdp:\n",
    "        uniq_values = ice.iloc[0,:]\n",
    "        ax.plot(uniq_values, avg_y - min_pdp_y,\n",
    "                alpha=pdp_alpha, linewidth=pdp_linewidth, c=pdp_color)\n",
    "\n",
    "    stop = time.time()\n",
    "    # print(f\"plot_ICE {stop - start:.3f}s\")\n",
    "\n",
    "\n",
    "def leaf_samples(rf, X:np.ndarray):\n",
    "    \"\"\"\n",
    "    Return a list of arrays where each array is the set of X sample indexes\n",
    "    residing in a single leaf of some tree in rf forest.\n",
    "    \"\"\"\n",
    "    ntrees = len(rf.estimators_)\n",
    "    leaf_ids = rf.apply(X) # which leaf does each X_i go to for each tree?\n",
    "    d = pd.DataFrame(leaf_ids, columns=[f\"tree{i}\" for i in range(ntrees)])\n",
    "    d = d.reset_index() # get 0..n-1 as column called index so we can do groupby\n",
    "    \"\"\"\n",
    "    d looks like:\n",
    "        index\ttree0\ttree1\ttree2\ttree3\ttree4\n",
    "    0\t0\t    8\t    3\t    4\t    4\t    3\n",
    "    1\t1\t    8\t    3\t    4\t    4\t    3\n",
    "    \"\"\"\n",
    "    leaf_samples = []\n",
    "    for i in range(ntrees):\n",
    "        \"\"\"\n",
    "        Each groupby gets a list of all X indexes associated with same leaf. 4 leaves would\n",
    "        get 4 arrays of X indexes; e.g.,\n",
    "        array([array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
    "               array([10, 11, 12, 13, 14, 15]), array([16, 17, 18, 19, 20]),\n",
    "               array([21, 22, 23, 24, 25, 26, 27, 28, 29]), ... )\n",
    "        \"\"\"\n",
    "        sample_idxs_in_leaf = d.groupby(f'tree{i}')['index'].apply(lambda x: x.values)\n",
    "        if len(sample_idxs_in_leaf) >= 2:\n",
    "            # can't detect changes with just one sample\n",
    "            leaf_samples.extend(sample_idxs_in_leaf)\n",
    "    return leaf_samples\n",
    "\n",
    "\n",
    "# Derived from dtreeviz\n",
    "def old_leaf_samples(tree_model, X):\n",
    "    \"\"\"\n",
    "    Return dictionary mapping node id to list of sample indexes in leaf nodes.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    tree = tree_model.tree_\n",
    "    children_left = tree.children_left\n",
    "\n",
    "    # Doc say: \"Return a node indicator matrix where non zero elements\n",
    "    #           indicates that the samples goes through the nodes.\"\n",
    "    dec_paths = tree_model.decision_path(X)\n",
    "\n",
    "    # each sample has path taken down tree\n",
    "    leaf_samples = []\n",
    "    node_to_leaves = defaultdict(list)\n",
    "    for sample_i, dec in enumerate(dec_paths):\n",
    "        _, nz_nodes = dec.nonzero()\n",
    "        for node_id in nz_nodes:\n",
    "            if children_left[node_id] == -1:  # is leaf?\n",
    "                node_to_leaves[node_id].append(sample_i)\n",
    "\n",
    "\n",
    "    stop = time.time()\n",
    "    # print(f\"leaf_samples {stop - start:.3f}s\")\n",
    "    return node_to_leaves\n",
    "\n",
    "\n",
    "def conjure_twoclass(X):\n",
    "    X_rand = df_scramble(X)\n",
    "    X_synth = pd.concat([X, X_rand], axis=0)\n",
    "    y_synth = np.concatenate([np.zeros(len(X)),\n",
    "                              np.ones(len(X_rand))], axis=0)\n",
    "    return X_synth, y_synth\n",
    "\n",
    "\n",
    "def hires_slopes_from_one_leaf(x:np.ndarray, y:np.ndarray):\n",
    "    start = time.time()\n",
    "    X = x.reshape(-1,1)\n",
    "    \"\"\"\n",
    "    Bootstrapping appears to be important, giving much better sine curve for weather().\n",
    "    min_samples_leaf=3 seems pretty good but min_samples_leaf=5 is smoother.\n",
    "    n_estimators=3 seems fine for sine curve.  Gotta keep cost down here as we might\n",
    "    call this a lot.\n",
    "    \"\"\"\n",
    "    rf = RandomForestRegressor(n_estimators=30, min_samples_leaf=5, bootstrap=True)\n",
    "    rf.fit(X, y)\n",
    "    leaves = leaf_samples(rf, X)\n",
    "    leaf_slopes = []\n",
    "    leaf_xranges = []\n",
    "    leaf_yranges = []\n",
    "    for samples in leaves:\n",
    "        leaf_x = X[samples]\n",
    "        leaf_y = y[samples]\n",
    "        r = (np.min(leaf_x), np.max(leaf_x))\n",
    "        if np.isclose(r[0], r[1]):\n",
    "            # print(f\"ignoring xleft=xright @ {r[0]}\")\n",
    "            continue\n",
    "        lm = LinearRegression()\n",
    "        lm.fit(leaf_x.reshape(-1, 1), leaf_y)\n",
    "        leaf_slopes.append(lm.coef_[0])\n",
    "        leaf_xranges.append(r)\n",
    "        leaf_yranges.append((leaf_y[0], leaf_y[-1]))\n",
    "    stop = time.time()\n",
    "    # print(f\"hires_slopes_from_one_leaf {stop - start:.3f}s\")\n",
    "    return leaf_xranges, leaf_yranges, leaf_slopes\n",
    "\n",
    "\n",
    "def collect_leaf_slopes(rf, X, y, colname, hires_threshold):\n",
    "    \"\"\"\n",
    "    For each leaf of each tree of the random forest rf (trained on all features\n",
    "    except colname), get the samples then isolate the column of interest X values\n",
    "    and the target y values. Perform a regression to get the slope of X[colname] vs y.\n",
    "    We don't need to subtract the minimum y value before regressing because\n",
    "    the slope won't be different. (We are ignoring the intercept of the regression line).\n",
    "\n",
    "    Return for each leaf, the range of X[colname], y at left/right of leaf range,\n",
    "    and associated slope for that range.\n",
    "\n",
    "    Currently, leaf_yranges is unused.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    leaf_slopes = []\n",
    "    leaf_xranges = []\n",
    "    leaf_yranges = []\n",
    "    leaves = leaf_samples(rf, X.drop(colname, axis=1))\n",
    "    for samples in leaves:\n",
    "        one_leaf_samples = X.iloc[samples]\n",
    "        leaf_x = one_leaf_samples[colname].values\n",
    "        leaf_y = y.iloc[samples].values\n",
    "        if len(samples)>hires_threshold:\n",
    "            print(f\"BIG {len(samples)}!!!\")\n",
    "            leaf_xranges_, leaf_yranges_, leaf_slopes_ = \\\n",
    "                hires_slopes_from_one_leaf(leaf_x, leaf_y)\n",
    "            leaf_slopes.extend(leaf_slopes_)\n",
    "            leaf_xranges.extend(leaf_xranges_)\n",
    "            leaf_yranges.extend(leaf_yranges_)\n",
    "            continue\n",
    "\n",
    "        r = (np.min(leaf_x), np.max(leaf_x))\n",
    "        if np.isclose(r[0], r[1]):\n",
    "            # print(f\"ignoring xleft=xright @ {r[0]}\")\n",
    "            continue\n",
    "        lm = LinearRegression()\n",
    "        lm.fit(leaf_x.reshape(-1, 1), leaf_y)\n",
    "        leaf_slopes.append(lm.coef_[0])\n",
    "        leaf_xranges.append(r)\n",
    "        leaf_yranges.append((leaf_y[0], leaf_y[-1]))\n",
    "    leaf_slopes = np.array(leaf_slopes)\n",
    "    leaf_xranges = np.array(leaf_xranges)\n",
    "    leaf_yranges = np.array(leaf_yranges)\n",
    "    stop = time.time()\n",
    "    print(f\"collect_leaf_slopes {stop - start:.3f}s\")\n",
    "    return leaf_xranges, leaf_yranges, leaf_slopes\n",
    "\n",
    "\n",
    "def catwise_leaves(rf, X, y, colname):\n",
    "    \"\"\"\n",
    "    Return a dataframe with the average y value for each category in each leaf\n",
    "    normalized by subtracting min avg y value from all categories.\n",
    "    The index has the complete category list. The columns are the y avg value changes\n",
    "    found in a single leaf. Each row represents a category level. E.g.,\n",
    "\n",
    "                       leaf0       leaf1\n",
    "        category\n",
    "        1         166.430176  186.796956\n",
    "        2         219.590349  176.448626\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    catcol = X[colname].astype('category').cat.as_ordered()\n",
    "    cats = catcol.cat.categories\n",
    "    leaf_histos = pd.DataFrame(index=cats)\n",
    "    leaf_histos.index.name = 'category'\n",
    "    ci = 0\n",
    "    Xy = pd.concat([X, y], axis=1)\n",
    "    leaves = leaf_samples(rf, X.drop(colname, axis=1))\n",
    "    for samples in leaves:\n",
    "        combined = Xy.iloc[samples]\n",
    "        # print(\"\\n\", combined)\n",
    "        histo = combined.groupby(colname).mean()\n",
    "        histo = histo.iloc[:,-1]\n",
    "#         print(histo)\n",
    "        #             print(histo - min_of_first_cat)\n",
    "        if len(histo) < 2:\n",
    "            # print(f\"ignoring len {len(histo)} cat leaf\")\n",
    "            continue\n",
    "        # record how much bump or drop we get per category above\n",
    "        # minimum change seen by any category (works even when all are negative)\n",
    "        # This assignment copies cat bumps to appropriate cat row using index\n",
    "        # leaving cats w/o representation as nan\n",
    "        relative_changes_per_cat = histo - np.min(histo.values)\n",
    "        leaf_histos['leaf' + str(ci)] = relative_changes_per_cat\n",
    "        ci += 1\n",
    "\n",
    "    # print(leaf_histos)\n",
    "    stop = time.time()\n",
    "    print(f\"catwise_leaves {stop - start:.3f}s\")\n",
    "    return leaf_histos\n",
    "\n",
    "\n",
    "def avg_slope_at_x(leaf_ranges, leaf_slopes):\n",
    "    start = time.time()\n",
    "    uniq_x = set(leaf_ranges[:, 0]).union(set(leaf_ranges[:, 1]))\n",
    "    uniq_x = np.array(sorted(uniq_x))\n",
    "    nx = len(uniq_x)\n",
    "    nslopes = len(leaf_slopes)\n",
    "    slopes = np.zeros(shape=(nx, nslopes))\n",
    "    i = 0  # leaf index; we get a line for each leaf\n",
    "    # collect the slope for each range (taken from a leaf) as collection of\n",
    "    # flat lines across the same x range\n",
    "    for r, slope in zip(leaf_ranges, leaf_slopes):\n",
    "        s = np.full(nx, slope) # s has value scope at all locations (flat line)\n",
    "        # now trim line so it's only valid in range r\n",
    "        s[np.where(uniq_x < r[0])] = np.nan\n",
    "        s[np.where(uniq_x > r[1])] = np.nan\n",
    "        slopes[:, i] = s\n",
    "        i += 1\n",
    "    # Now average horiz across the matrix, averaging within each range\n",
    "    sum_at_x = np.nansum(slopes, axis=1)\n",
    "    missing_values_at_x = np.isnan(slopes).sum(axis=1)\n",
    "    count_at_x = nslopes - missing_values_at_x\n",
    "    # The value could be genuinely zero so we use nan not 0 for out-of-range\n",
    "    avg_slope_at_x = sum_at_x / count_at_x\n",
    "\n",
    "    stop = time.time()\n",
    "    # print(f\"avg_slope_at_x {stop - start:.3f}s\")\n",
    "    return uniq_x, avg_slope_at_x\n",
    "\n",
    "\n",
    "def lm_partial_plot(X, y, colname, targetname,ax=None):\n",
    "    r_col = LinearRegression()\n",
    "    r_col.fit(X[[colname]], y)\n",
    "    ax.scatter(X[colname], y, alpha=.12)\n",
    "    ax.set_xlabel(colname)\n",
    "    ax.set_ylabel(targetname)\n",
    "    ax.set_title(targetname+\" vs \"+colname)\n",
    "    col = X[colname]\n",
    "    y_pred_hp = r_col.predict(col.values.reshape(-1, 1))\n",
    "    ax.plot(col, y_pred_hp, \":\", linewidth=1, c='red', label='OLS y ~ ENG')\n",
    "    r = LinearRegression()\n",
    "    r.fit(X, y)\n",
    "    xhp = np.linspace(min(col), max(col), num=100)\n",
    "    ci = X.columns.get_loc(colname)\n",
    "    ax.plot(xhp, xhp * r.coef_[ci] + r_col.intercept_, linewidth=1, c='orange', label=\"Beta_ENG\")\n",
    "    left30 = xhp[int(len(xhp) * .3)]\n",
    "    ax.text(left30, left30*r.coef_[ci] + r_col.intercept_, f\"slope={r.coef_[ci]:.3f}\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def partial_plot(X, y, colname, targetname=None,\n",
    "                 ax=None,\n",
    "                 ntrees=30,\n",
    "                 min_samples_leaf=2,\n",
    "                 alpha=.05,\n",
    "                 hires_threshold=20,\n",
    "                 xrange=None,\n",
    "                 yrange=None,\n",
    "                 show_derivative=False):\n",
    "    rf = RandomForestRegressor(n_estimators=ntrees,\n",
    "                               min_samples_leaf=min_samples_leaf,\n",
    "                               # max_features=1.0,\n",
    "                               # bootstrap=False,\n",
    "                               oob_score=False)\n",
    "    rf.fit(X.drop(colname, axis=1), y)\n",
    "    # print(f\"\\nModel wo {colname} OOB R^2 {rf.oob_score_:.5f}\")\n",
    "    leaf_xranges, leaf_yranges, leaf_slopes = collect_leaf_slopes(rf, X, y, colname, hires_threshold=hires_threshold)\n",
    "    uniq_x, slope_at_x = avg_slope_at_x(leaf_xranges, leaf_slopes)\n",
    "    # print(f'uniq_x = [{\", \".join([f\"{x:4.1f}\" for x in uniq_x])}]')\n",
    "    # print(f'slopes = [{\", \".join([f\"{s:4.1f}\" for s in slope_at_x])}]')\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "\n",
    "    curve = cumtrapz(slope_at_x, x=uniq_x)          # we lose one value here\n",
    "    curve = np.concatenate([np.array([0]), curve])  # add back the 0 we lost\n",
    "\n",
    "    # if 0 is in x feature and not on left/right edge, get y at 0\n",
    "    # and shift so that is x,y 0 point.\n",
    "    # nx = len(uniq_x)\n",
    "    # if uniq_x[int(nx*0.05)]<0 or uniq_x[-int(nx*0.05)]>0:\n",
    "    #     closest_x_to_0 = np.abs(uniq_x - 0.0).argmin()\n",
    "    #     y_offset = curve[closest_x_to_0]\n",
    "    #     curve -= y_offset  # shift\n",
    "    # Nah. starting with 0 is best\n",
    "\n",
    "    ax.scatter(uniq_x, curve,\n",
    "               s=3, alpha=1,\n",
    "               c='black', label=\"Avg piecewise linear\")\n",
    "\n",
    "    segments = []\n",
    "    for xr, yr, slope in zip(leaf_xranges, leaf_yranges, leaf_slopes):\n",
    "        delta = slope * (xr[1] - xr[0])\n",
    "        closest_x_i = np.abs(uniq_x - xr[0]).argmin() # find curve point for xr[0]\n",
    "        y_offset = curve[closest_x_i]\n",
    "        # one_line = [(xr[0],y_offset+yr[0]), (xr[1], y_offset+delta+yr[0])]\n",
    "        one_line = [(xr[0],y_offset), (xr[1], y_offset+delta)]\n",
    "        segments.append( one_line )\n",
    "\n",
    "    lines = LineCollection(segments, alpha=alpha, color='#9CD1E3', linewidth=1)\n",
    "    if xrange is not None:\n",
    "        ax.set_xlim(*xrange)\n",
    "    else:\n",
    "        ax.set_xlim(min(uniq_x),max(uniq_x))\n",
    "    if yrange is not None:\n",
    "        ax.set_ylim(*yrange)\n",
    "    ax.add_collection(lines)\n",
    "\n",
    "    ax.set_xlabel(colname)\n",
    "    ax.set_ylabel(targetname)\n",
    "    if hasattr(rf, 'oob_score_'):\n",
    "        ax.set_title(f\"Effect of {colname} on {targetname} in similar regions\\nOOB R^2 {rf.oob_score_:.3f}\")\n",
    "    else:\n",
    "        ax.set_title(f\"Effect of {colname} on {targetname} in similar regions\")\n",
    "\n",
    "    if show_derivative:\n",
    "        other = ax.twinx()\n",
    "        other.set_ylabel(\"Partial derivative\", fontdict={\"color\":'#f46d43'})\n",
    "        other.plot(uniq_x, slope_at_x, linewidth=1, c='#f46d43', alpha=.5)\n",
    "        other.set_ylim(min(slope_at_x),max(slope_at_x))\n",
    "        other.tick_params(axis='y', colors='#f46d43')\n",
    "        m = np.mean(slope_at_x)\n",
    "        mx = np.max(uniq_x)\n",
    "        mnx = np.min(uniq_x)\n",
    "        other.plot(mx-(mx-mnx)*0.02, m, marker='>', c='#f46d43')\n",
    "\n",
    "\n",
    "def cat_partial_plot(X, y, colname, targetname,\n",
    "                     cats=None,\n",
    "                     ax=None,\n",
    "                     sort='ascending',\n",
    "                     ntrees=30, min_samples_leaf=5,\n",
    "                     alpha=.03,\n",
    "                     yrange=None):\n",
    "    rf = RandomForestRegressor(n_estimators=ntrees, min_samples_leaf=min_samples_leaf, oob_score=True)\n",
    "    rf.fit(X.drop(colname, axis=1), y)\n",
    "    print(f\"Model wo {colname} OOB R^2 {rf.oob_score_:.5f}\")\n",
    "    leaf_histos = catwise_leaves(rf, X, y, colname)\n",
    "    sum_per_cat = np.sum(leaf_histos, axis=1)\n",
    "    nonmissing_count_per_cat = len(leaf_histos.columns) - np.isnan(leaf_histos).sum(axis=1)\n",
    "    avg_per_cat = sum_per_cat / nonmissing_count_per_cat\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "    ncats = len(cats)\n",
    "    nleaves = len(leaf_histos.columns)\n",
    "\n",
    "    sort_indexes = range(ncats)\n",
    "    if sort == 'ascending':\n",
    "        sort_indexes = avg_per_cat.argsort()\n",
    "        cats = cats[sort_indexes]\n",
    "    elif sort == 'descending':\n",
    "        sort_indexes = avg_per_cat.argsort()[::-1]  # reversed\n",
    "        cats = cats[sort_indexes]\n",
    "\n",
    "    min_value = np.min(avg_per_cat)\n",
    "\n",
    "    xloc = 1\n",
    "    sigma = .02\n",
    "    mu = 0\n",
    "    x_noise = np.random.normal(mu, sigma, size=nleaves)\n",
    "    for i in sort_indexes:\n",
    "        ax.scatter(x_noise + xloc, leaf_histos.iloc[i]-min_value,\n",
    "                   alpha=alpha, marker='o', s=10,\n",
    "                   c='#9CD1E3')\n",
    "        ax.plot([xloc - .1, xloc + .1], [avg_per_cat.iloc[i]-min_value] * 2,\n",
    "                c='black', linewidth=2)\n",
    "        xloc += 1\n",
    "    ax.set_xticks(range(1, ncats + 1))\n",
    "    ax.set_xticklabels(cats)\n",
    "\n",
    "    ax.set_xlabel(colname)\n",
    "    ax.set_ylabel(targetname)\n",
    "    ax.set_title(f\"Effect of {colname} on {targetname} in similar regions\")\n",
    "\n",
    "    if yrange is not None:\n",
    "        ax.set_ylim(*yrange)\n",
    "\n",
    "\n",
    "def cars():\n",
    "    df_cars = pd.read_csv(\"/Users/parrt/github/dtreeviz/testing/data/cars.csv\")\n",
    "    X = df_cars[['ENG', 'WGT']]\n",
    "    y = df_cars['MPG']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12,8))\n",
    "    lm_partial_plot(X, y, 'ENG', 'MPG', ax=axes[0,0])\n",
    "    partial_plot(X, y, 'ENG', 'MPG', ax=axes[0,1], show_derivative=True, yrange=(-20,20))\n",
    "\n",
    "    lm_partial_plot(X, y, 'WGT', 'MPG', ax=axes[1,0])\n",
    "    partial_plot(X, y, 'WGT', 'MPG', ax=axes[1,1], show_derivative=True, yrange=(-20,20))\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=50, min_samples_leaf=1, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "    ice = ICE_predict(rf, X, 'ENG', 'MPG', nlines=50, numx=None)\n",
    "    plot_ICE(ice, 'ENG', 'MPG', ax=axes[0, 2], yrange=(-20,20))\n",
    "    ice = ICE_predict(rf, X, 'WGT', 'MPG', nlines=50, numx=100)\n",
    "    plot_ICE(ice, 'WGT', 'MPG', ax=axes[1, 2], yrange=(-20,20))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def rent():\n",
    "    df_rent = pd.read_csv(\"/Users/parrt/github/mlbook-private/data/rent-ideal.csv\")\n",
    "    df_rent = df_rent.sample(n=2000)\n",
    "    X = df_rent.drop('price', axis=1)\n",
    "    y = df_rent['price']\n",
    "\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(8,16))\n",
    "    partial_plot(X, y, 'bedrooms', 'price', ax=axes[0,0], alpha=.03, yrange=(0,3000))\n",
    "    partial_plot(X, y, 'bathrooms', 'price', ax=axes[1,0], alpha=.03, yrange=(0,5000))\n",
    "    partial_plot(X, y, 'latitude', 'price', ax=axes[2,0], alpha=.03, yrange=(0,1700))\n",
    "    partial_plot(X, y, 'longitude', 'price', ax=axes[3,0], alpha=.03, yrange=(-3000,250))\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=1, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    # rf = Lasso()\n",
    "    # rf.fit(X, y)\n",
    "\n",
    "    ice = ICE_predict(rf, X, 'bedrooms', 'price')\n",
    "    plot_ICE(ice, 'bedrooms', 'price', ax=axes[0, 1], alpha=.05, yrange=(0,3000))\n",
    "    ice = ICE_predict(rf, X, 'bathrooms', 'price')\n",
    "    plot_ICE(ice, 'bathrooms', 'price', alpha=.05, ax=axes[1, 1])\n",
    "    ice = ICE_predict(rf, X, 'latitude', 'price')\n",
    "    plot_ICE(ice, 'latitude', 'price', ax=axes[2, 1], alpha=.05, yrange=(0,1700))\n",
    "    ice = ICE_predict(rf, X, 'longitude', 'price')\n",
    "    plot_ICE(ice, 'longitude', 'price', ax=axes[3, 1], alpha=.05, yrange=(-750,3000))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"/tmp/rent.svg\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def weight():\n",
    "    df_raw = toy_weight_data(2000)\n",
    "    df = df_raw.copy()\n",
    "    catencoders = df_string_to_cat(df)\n",
    "    df_cat_to_catcode(df)\n",
    "    df['pregnant'] = df['pregnant'].astype(int)\n",
    "    X = df.drop('weight', axis=1)\n",
    "    y = df['weight']\n",
    "\n",
    "    fig, axes = plt.subplots(5, 2, figsize=(8,16), gridspec_kw = {'height_ratios':[.2,3,3,3,3]})\n",
    "\n",
    "    axes[0,0].get_xaxis().set_visible(False)\n",
    "    axes[0,1].get_xaxis().set_visible(False)\n",
    "    axes[0,0].axis('off')\n",
    "    axes[0,1].axis('off')\n",
    "\n",
    "    partial_plot(X, y, 'education', 'weight', ax=axes[1][0],\n",
    "                 yrange=(-12,0)\n",
    "                 )\n",
    "    partial_plot(X, y, 'height', 'weight', ax=axes[2][0],\n",
    "                 yrange=(0,160)\n",
    "                 )\n",
    "    cat_partial_plot(X, y, 'sex', 'weight', ax=axes[3][0], ntrees=50,\n",
    "                     alpha=.2,\n",
    "                     cats=df_raw['sex'].unique(),\n",
    "                     yrange=(0,5)\n",
    "                     )\n",
    "    cat_partial_plot(X, y, 'pregnant', 'weight', ax=axes[4][0], ntrees=50,\n",
    "                     alpha=.2,\n",
    "                     cats=df_raw['pregnant'].unique(),\n",
    "                     yrange=(0,10)\n",
    "                     )\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=1, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    if True:\n",
    "        ice = ICE_predict(rf, X, 'education', 'weight')\n",
    "        plot_ICE(ice, 'education', 'weight', ax=axes[1, 1], yrange=(-12, 0))\n",
    "        ice = ICE_predict(rf, X, 'height', 'weight')\n",
    "        plot_ICE(ice, 'height', 'weight', ax=axes[2, 1], yrange=(0, 160))\n",
    "        ice = ICE_predict(rf, X, 'sex', 'weight')\n",
    "        plot_ICE(ice, 'sex', 'weight', ax=axes[3,1], yrange=(0,5), cats=df_raw['sex'].unique())\n",
    "        ice = ICE_predict(rf, X, 'pregnant', 'weight')\n",
    "        plot_ICE(ice, 'pregnant', 'weight', ax=axes[4,1], yrange=(0,10), cats=df_raw['pregnant'].unique())\n",
    "\n",
    "    fig.suptitle(\"weight = 120 + 10*(height-min(height)) + 10*pregnant - 1.2*education\", size=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"/tmp/t.svg\")\n",
    "    plt.show()\n",
    "\n",
    "def weather():\n",
    "    df_raw = toy_weather_data()\n",
    "    df = df_raw.copy()\n",
    "    catencoders = df_string_to_cat(df)\n",
    "    print(catencoders)\n",
    "    df_cat_to_catcode(df)\n",
    "    X = df.drop('temperature', axis=1)\n",
    "    y = df['temperature']\n",
    "\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(8,8), gridspec_kw = {'height_ratios':[.2,3,3,3]})\n",
    "\n",
    "    axes[0,0].get_xaxis().set_visible(False)\n",
    "    axes[0,1].get_xaxis().set_visible(False)\n",
    "    axes[0,0].axis('off')\n",
    "    axes[0,1].axis('off')\n",
    "\n",
    "    \"\"\"\n",
    "    The scale diff between states, obscures the sinusoidal nature of the\n",
    "    dayofyear vs temp plot. With noise N(0,5) gotta zoom in -3,3 on mine too.\n",
    "    otherwise, smooth quasilinear plot with lots of bristles showing volatility.\n",
    "    Flip to N(-5,5) which is more realistic and we see sinusoid for both, even at\n",
    "    scale. yep, the N(0,5) was obscuring sine for both. \n",
    "    \"\"\"\n",
    "    partial_plot(X, y, 'dayofyear', 'temperature', ax=axes[1][0],\n",
    "                 ntrees=50, min_samples_leaf=2, yrange=(-5,5))\n",
    "    cat_partial_plot(X, y, 'state', 'temperature', cats=catencoders['state'], ax=axes[2][0])#, yrange=(0,160))\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=30, min_samples_leaf=1, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    ice = ICE_predict(rf, X, 'dayofyear', 'temperature')\n",
    "    plot_ICE(ice, 'dayofyear', 'temperature', ax=axes[1, 1])  #, yrange=(-12,0))\n",
    "\n",
    "    ice = ICE_predict(rf, X, 'state', 'temperature')\n",
    "    plot_ICE(ice, 'state', 'temperature', cats=catencoders['state'], ax=axes[2, 1])  #, yrange=(-12,0))\n",
    "\n",
    "    df = df_raw.copy()\n",
    "    axes[3, 0].plot(df.loc[df['state'] == 'CA', 'dayofyear'],\n",
    "             df.loc[df['state'] == 'CA', 'temperature'], label=\"CA\")\n",
    "    axes[3, 0].plot(df.loc[df['state'] == 'CO', 'dayofyear'],\n",
    "             df.loc[df['state'] == 'CO', 'temperature'], label=\"CO\")\n",
    "    axes[3, 0].plot(df.loc[df['state'] == 'AZ', 'dayofyear'],\n",
    "             df.loc[df['state'] == 'AZ', 'temperature'], label=\"AZ\")\n",
    "    axes[3, 0].plot(df.loc[df['state'] == 'WA', 'dayofyear'],\n",
    "             df.loc[df['state'] == 'WA', 'temperature'], label=\"WA\")\n",
    "    axes[3, 0].legend()\n",
    "    axes[3,0].set_title('Raw data')\n",
    "    axes[3, 0].set_ylabel('Temperature')\n",
    "    axes[3, 0].set_xlabel('Dataframe row index')\n",
    "\n",
    "    rtreeviz_univar(axes[3,1],\n",
    "                    X['state'], y,\n",
    "                    feature_name='state',\n",
    "                    target_name='y',\n",
    "                    min_samples_leaf=2,\n",
    "                    fontsize=10)\n",
    "    axes[3,1].set_title(f'state space partition with min_samples_leaf={2}')\n",
    "    axes[3,1].set_xlabel(\"state\")\n",
    "    axes[3,1].set_ylabel(\"y\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"/tmp/weather.svg\")\n",
    "    plt.show()\n",
    "\n",
    "def interaction(f, n=100):\n",
    "    df,eqn,yrange = f(n=n)\n",
    "\n",
    "    X = df.drop('y', axis=1)\n",
    "    y = df['y']\n",
    "    min_samples_leaf = 2\n",
    "\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(10,13))\n",
    "\n",
    "    axes[0,0].plot(range(len(df)), df['x1'], label=\"x1\")\n",
    "    axes[0,0].plot(range(len(df)), df['x2'], label=\"x2\")\n",
    "    axes[0,0].plot(range(len(df)), df['y'], label=\"y\")\n",
    "    axes[0, 0].set_xlabel(\"df row index\")\n",
    "    axes[0, 0].set_ylabel(\"df value\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].set_title(f\"Raw data; {eqn}\")\n",
    "\n",
    "    # axes[0,1].get_xaxis().set_visible(False)\n",
    "    # axes[0,1].axis('off')\n",
    "\n",
    "    rtreeviz_univar(axes[0,1],\n",
    "                    df['x1'], y,\n",
    "                    feature_name='x1',\n",
    "                    target_name='y',\n",
    "                    min_samples_leaf=min_samples_leaf,\n",
    "                    fontsize=10)\n",
    "    axes[0,1].set_title(f'x1 space partition with min_samples_leaf={min_samples_leaf}')\n",
    "    axes[0,1].set_xlabel(\"x1\")\n",
    "    axes[0,1].set_ylabel(\"y\")\n",
    "\n",
    "    # print(df)\n",
    "    # print(f\"x1 = {df['x1'].values.tolist()}\")\n",
    "    # print(f\"x2 = {df['x2'].values.tolist()}\")\n",
    "    # print(f\"y = {df['y'].values.tolist()}\")\n",
    "    axes[1,0].scatter(df['x1'], y)\n",
    "    axes[1,0].set_xlabel(\"x1\")\n",
    "    axes[1,0].set_ylabel(\"y\")\n",
    "    axes[1,1].scatter(df['x2'], y)\n",
    "    axes[1,1].set_xlabel(\"x2\")\n",
    "    axes[1,1].set_ylabel(\"y\")\n",
    "\n",
    "    partial_plot(X, y, 'x1', 'y', ax=axes[2][0],\n",
    "                 ntrees=30, min_samples_leaf=min_samples_leaf, yrange=yrange,\n",
    "                 show_derivative=True)\n",
    "    # partial_plot(X, y, 'education', 'weight', ntrees=20, min_samples_leaf=7, alpha=.2)\n",
    "    partial_plot(X, y, 'x2', 'y', ax=axes[3][0], min_samples_leaf=min_samples_leaf,\n",
    "                 ntrees=30, yrange=yrange,\n",
    "                 show_derivative=True)\n",
    "    # cat_partial_plot(axes[2][0], X, y, 'sex', 'weight', ntrees=50, min_samples_leaf=7, cats=df_raw['sex'].unique(), yrange=(0,2))\n",
    "    # cat_partial_plot(axes[3][0], X, y, 'pregnant', 'weight', ntrees=50, min_samples_leaf=7, cats=df_raw['pregnant'].unique(), yrange=(0,10))\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=1, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    ice = ICE_predict(rf, X, 'x1', 'y', numx=None)\n",
    "    plot_ICE(ice, 'x1', 'y', ax=axes[2, 1], yrange=yrange)\n",
    "    ice = ICE_predict(rf, X, 'x2', 'y', numx=None)\n",
    "    plot_ICE(ice, 'x2', 'y', ax=axes[3, 1], yrange=yrange)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"/tmp/interaction-{n}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def wine():\n",
    "    wine = load_wine()\n",
    "\n",
    "\n",
    "def bigX():\n",
    "    def bigX_data(n):\n",
    "        x1 = np.random.uniform(-1, 1, size=n)\n",
    "        x2 = np.random.uniform(-1, 1, size=n)\n",
    "        x3 = np.random.uniform(-1, 1, size=n)\n",
    "\n",
    "        y = 0.2 * x1 - 5 * x2 + 10 * x2 * np.where(x3 >= 0, 1, 0) + np.random.normal(0, 1, size=n)\n",
    "        df = pd.DataFrame()\n",
    "        df['x1'] = x1\n",
    "        df['x2'] = x2\n",
    "        df['x3'] = x3\n",
    "        df['y'] = y\n",
    "        return df\n",
    "\n",
    "    n = 1000\n",
    "    df = bigX_data(n=n)\n",
    "    X = df.drop('y', axis=1)\n",
    "    y = df['y']\n",
    "\n",
    "    fig, axes = plt.subplots(5, 2, figsize=(11, 14), gridspec_kw = {'height_ratios':[.1,4,4,4,4]})\n",
    "\n",
    "    axes[0, 0].get_xaxis().set_visible(False)\n",
    "    axes[0, 1].get_xaxis().set_visible(False)\n",
    "    axes[0, 0].axis('off')\n",
    "    axes[0, 1].axis('off')\n",
    "\n",
    "    axes[1,0].scatter(df['x3'], y, s=5, alpha=.7)\n",
    "    axes[1,0].set_xlabel('x3')\n",
    "    axes[1,0].set_ylabel('y')\n",
    "\n",
    "    axes[1,1].scatter(df['x2'], df['y'], s=5, alpha=.7)\n",
    "    axes[1,1].set_ylabel('y')\n",
    "    axes[1,1].set_xlabel('x2')\n",
    "\n",
    "    # Partial deriv is just 0.2 so this is correct. flat deriv curve, net effect line at slope .2\n",
    "    # ICE is way too shallow and not line at n=1000 even\n",
    "    partial_plot(X, y, 'x1', 'y', ax=axes[2,0])\n",
    "    # Partial deriv wrt x2 is -5 plus 10 about half the time so about 0\n",
    "    # Should not expect a criss-cross like ICE since deriv of 1_x3>=0 is 0 everywhere\n",
    "    # wrt to any x, even x3. x2 *is* affecting y BUT the net effect at any spot\n",
    "    # is what we care about and that's 0. Just because marginal x2 vs y shows non-\n",
    "    # random plot doesn't mean that x2's net effect is nonzero. We are trying to\n",
    "    # strip away x1/x3's effect upon y. When we do, x2 has no effect on y.\n",
    "    # Key is asking right question. Don't look at marginal plot and say obvious.\n",
    "    # Ask what is net effect at every x2? 0.\n",
    "    partial_plot(X, y, 'x2', 'y', ax=axes[3,0], yrange=(-4,4))\n",
    "    # Partial deriv wrt x3 of 1_x3>=0 is 0 everywhere so result must be 0\n",
    "    partial_plot(X, y, 'x3', 'y', ax=axes[4,0], yrange=(-4,4))\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=1, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "    print(f\"RF OOB {rf.oob_score_}\")\n",
    "\n",
    "    ice = ICE_predict(rf, X, 'x1', 'y', numx=10)\n",
    "    plot_ICE(ice, 'x1', 'y', ax=axes[2, 1], yrange=(-.05,.5))\n",
    "\n",
    "    ice = ICE_predict(rf, X, 'x2', 'y', numx=10)\n",
    "    plot_ICE(ice, 'x2', 'y', ax=axes[3, 1])\n",
    "\n",
    "    ice = ICE_predict(rf, X, 'x3', 'y', numx=10)\n",
    "    plot_ICE(ice, 'x3', 'y', ax=axes[4, 1])\n",
    "\n",
    "    fig.suptitle(\"$y = 0.2x_1 - 5x_2 + 10x_2\\mathbb{1}_{x_3 \\geq 0} + \\epsilon$\\n$x_1, x_2, x_3$ are U(-1,1)\\nSample size \"+str(n))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def boston():\n",
    "    df = pd.read_csv('/Users/parrt/github/random-forest-importances/notebooks/data/boston.csv')\n",
    "    X = df.drop('medv', axis=1)\n",
    "    y = df['medv']\n",
    "\n",
    "    \"\"\"\n",
    "    Wow. My net effect plots look kinda like the centered ICE c-ICE plots\n",
    "    from paper: https://arxiv.org/pdf/1309.6392.pdf\n",
    "    Mine are way smoother.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(8, 10), gridspec_kw = {'height_ratios':[.05,4,4]})\n",
    "\n",
    "    axes[0, 0].get_xaxis().set_visible(False)\n",
    "    axes[0, 1].get_xaxis().set_visible(False)\n",
    "    axes[0, 0].axis('off')\n",
    "    axes[0, 1].axis('off')\n",
    "\n",
    "    axes[1,0].scatter(df['age'], y, s=5, alpha=.7)\n",
    "    axes[1,0].set_xlabel('age')\n",
    "    axes[1,0].set_ylabel('median home value')\n",
    "\n",
    "    partial_plot(X, y, 'age', 'medv', ax=axes[2,0], show_derivative=True, yrange=(-20,20))\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "    print(f\"RF OOB {rf.oob_score_}\")\n",
    "\n",
    "    ice = ICE_predict(rf, X, 'age', 'medv', numx=10)\n",
    "    plot_ICE(ice, 'age', 'medv', ax=axes[2, 1], yrange=(-20,20))\n",
    "\n",
    "    fig.suptitle(f\"Boston housing data {len(X)} training samples\\nRandom Forest ntrees=100\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def additive_assessment():\n",
    "    def data(n):\n",
    "        x1 = np.random.uniform(-3, 3, size=n)\n",
    "        x2 = np.random.uniform(-3, 3, size=n)\n",
    "        x3 = np.random.uniform(-3, 3, size=n)\n",
    "        x4 = np.random.uniform(-3, 3, size=n)\n",
    "\n",
    "        y = x1*x1 + x2 + x3 + x4# + np.random.normal(0, 1, size=n)\n",
    "        df = pd.DataFrame()\n",
    "        df['x1'] = x1\n",
    "        df['x2'] = x2\n",
    "        df['x3'] = x3\n",
    "        df['x4'] = x4\n",
    "        df['y'] = y\n",
    "        return df\n",
    "\n",
    "    n = 500\n",
    "    df = data(n=n)\n",
    "    X = df.drop('y', axis=1)\n",
    "    y = df['y']\n",
    "\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(11, 14), gridspec_kw = {'height_ratios':[.1,4,4,4]})\n",
    "\n",
    "    axes[0, 0].get_xaxis().set_visible(False)\n",
    "    axes[0, 0].axis('off')\n",
    "    axes[0, 1].get_xaxis().set_visible(False)\n",
    "    axes[0, 1].axis('off')\n",
    "\n",
    "    # axes[1,0].scatter(df['x1'], y, s=5, alpha=.7)\n",
    "    # axes[1,0].set_xlabel('x1')\n",
    "    # axes[1,0].set_ylabel('y')\n",
    "\n",
    "    \"\"\"\n",
    "    When we have too many samples in leaf, we don't get enough detail / points\n",
    "    near zero and it looks like line not parabola.\n",
    "    \n",
    "    Mine looks like parabola but U(-3,3) gives max values of 2.5ish for -3 and 3\n",
    "    whereas PDP gives 8 for -3 and 3. n=1000 seems a bit shifted but n=2000 gets\n",
    "    center/base of parabola correctly at x1=0.\n",
    "    \n",
    "    When one is shallow like a line then leaf might get lots of values and\n",
    "    therefore bad slope estimate. Just two vars like y=x1^2 + x2 shows us biased\n",
    "    too low for x1. \n",
    "    \n",
    "    oh shit. the RF is bootstrapping and missing lots of values. try all.\n",
    "    Make max_features=1.0 too. we don't care about overfitting here, do we?\n",
    "    \n",
    "    Turning off bootstrap (no replace, but same sample size) gets much taller\n",
    "    parabola, though max_features=1.0 didn't do much.\n",
    "    \"\"\"\n",
    "    min_samples_leaf = 2\n",
    "    rtreeviz_univar(axes[1,0],\n",
    "                    df['x1'], y,\n",
    "                    feature_name='x1',\n",
    "                    target_name='y',\n",
    "                    min_samples_leaf=min_samples_leaf,\n",
    "                    fontsize=10)\n",
    "    axes[1,0].set_title(f'x1 space partition with min_samples_leaf={min_samples_leaf}')\n",
    "    axes[1,0].set_xlabel(\"x1\")\n",
    "    axes[1,0].set_ylabel(\"y\")\n",
    "\n",
    "    rtreeviz_univar(axes[1,1],\n",
    "                    df['x2'], y,\n",
    "                    feature_name='x2',\n",
    "                    target_name='y',\n",
    "                    min_samples_leaf=min_samples_leaf,\n",
    "                    fontsize=10)\n",
    "    axes[1,1].set_title(f'x2 space partition with min_samples_leaf={min_samples_leaf}')\n",
    "    axes[1,1].set_xlabel(\"x2\")\n",
    "    axes[1,1].set_ylabel(\"y\")\n",
    "\n",
    "    partial_plot(X, y, 'x1', 'y', ax=axes[2,0], min_samples_leaf=min_samples_leaf)\n",
    "    partial_plot(X, y, 'x2', 'y', ax=axes[3,0], min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=1, oob_score=True)\n",
    "    rf.fit(X, y)\n",
    "    print(f\"RF OOB {rf.oob_score_}\")\n",
    "\n",
    "    ice = ICE_predict(rf, X, 'x1', 'y', numx=20)\n",
    "    plot_ICE(ice, 'x1', 'y', ax=axes[2, 1])\n",
    "\n",
    "    ice = ICE_predict(rf, X, 'x2', 'y', numx=20)\n",
    "    plot_ICE(ice, 'x2', 'y', ax=axes[3, 1], yrange=(-3,3))\n",
    "\n",
    "    fig.suptitle(\"$y = x_1^2 + x_2 + x_3 + x_4$\\n$x_1, x_2, x_3$ are U(-3,3)\\nSample size \"+str(n))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # cars()\n",
    "    # rent()\n",
    "    weight()\n",
    "    # weather()\n",
    "    # interaction(toy_x1_times_x2_data)\n",
    "    # interaction(toy_2x1_times_3x2_data)\n",
    "    # bigX()\n",
    "    # boston()\n",
    "    # additive_assessment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
